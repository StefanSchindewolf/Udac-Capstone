{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Data ETL\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2 Explore and assess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Overview\n",
    "In this section we will explore the data, idenfify and implement necessary transformations and finally export the data to S3 in parquet format.\n",
    "\n",
    "Subsections of this Notebook:\n",
    "* Preparations (imports, file locations, etc.)\n",
    "* Dataframe schema analysis\n",
    "* Explorative analysis of the data\n",
    "* Dataframe transformation steps\n",
    "* Defining a new schema\n",
    "* Writing data to S3 in Parquet format\n",
    "* Examples of analyzing table contents using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from os.path import getsize\n",
    "from nb_helpers import summarize_data, get_sas_definitions, read_sas_in_chunks, read_csv_print, print_stat, non_iso_date_change, change_nullables\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s %(levelname)s \\t %(message)s ',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "log = logging.getLogger('log')\n",
    "\n",
    "# Improve view\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Reading data files into Spark Dataframes\n",
    "The next cell contains the file locations for each dataset, you may adjust it if required. Also note the configuration of the sample size which will improve performance while making some basic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Source file locations\n",
    "dem_file = 'us-cities-demographics.csv'\n",
    "imm_file = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "air_file = 'airport-codes_csv.csv'\n",
    "\n",
    "# Schema file locations\n",
    "air_schema_file = 'air_schema.json'\n",
    "dem_schema_file = 'dem_schema.json'\n",
    "imm_schema_file = 'imm_schema.json'\n",
    "\n",
    "\n",
    "# If working on a sample of the large immigration dataset, set your sample size here:\n",
    "sample_size = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "During my analysis I changed the schema of each dataset imported and based on the transformations done created a new schema definition. The definition is stored in a JSON file.\n",
    "\n",
    "The data will be **imported again** using those schema **at the end** of the \"Explore and Assess\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we setup the Spark Session and read all three files into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  3096313  lines from  ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "Reading  2891  lines from  us-cities-demographics.csv\n",
      "Reading  55075  lines from  airport-codes_csv.csv\n",
      "Operation runtime  0:00:43.137382\n"
     ]
    }
   ],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "############# TEST option(\"inferSchema\", \"true\")\n",
    "start_time = datetime.now()\n",
    "imm_df = spark.read.format('com.github.saurfang.sas.spark').option(\"inferSchema\",\"true\").load(imm_file)\n",
    "imm_rows = imm_df.count()\n",
    "air_df = spark.read.csv(air_file, header=True)\n",
    "air_rows = air_df.count()\n",
    "dem_df = spark.read.csv(dem_file, header=True)\n",
    "dem_rows = dem_df.count()\n",
    "df_list = {'imm_df': imm_df, 'air_df': air_df, 'dem_df': dem_df}\n",
    "imm_full = imm_df\n",
    "elapse = datetime.now() - start_time\n",
    "print('Reading ', imm_rows , ' lines from ', imm_file)\n",
    "print('Reading ', dem_rows, ' lines from ', dem_file)\n",
    "print('Reading ', air_rows, ' lines from ', air_file)\n",
    "print('Operation runtime ', elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Explore and Describe data\n",
    "This section contains the following subsections:\n",
    "* Spark schema analysis\n",
    "* Spark dataframe descriptions analysis\n",
    "\n",
    "Before we start this section we are naming each dataframe and reducing immigration data to a sample of the complete dataframe, keeping the full set in variable `imm_full` (the sample size being configured at the beginning of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_df = imm_full.sample(sample_size)\n",
    "imm_df.name = 'i94 immigration data'\n",
    "dem_df.name = 'city demographics'\n",
    "air_df.name = 'airport codes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note:**\n",
    "\n",
    "    * We want to keep track of transformation steps\n",
    "    * So if a required transformation is identified, it will be assigned a number like \"TFA-x\"\n",
    "    * Where \"TFA\" stands for transformation action (plus a number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Display schemas created automatically by Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imm_df.printSchema()\n",
    "air_df.printSchema()\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note** (TFA-1) Several columns in the i94 dataframe were infered as double while they should be of data type integer. Those types should be changed (see `cicid, i94yr,...`).\n",
    "\n",
    "**Note** (TFA-2) Several columns in airport and demographic data were infered as string where they should be either double or integer (see airport data, `elevation_ft` or demographic data `median_age`\n",
    "\n",
    "**Note** (TFA-3) Demographics data file was imported as a one-column file due to ';' being used as delimiter. We read the file again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dem_df = spark.read.csv(dem_file, header=True, sep=';')\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note**: (TFA-4) For easier handling we strip columns of spaces, apply lower case and replace inner spaces and '-' with underscore '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def trim_col_headers(df):\n",
    "    for col in df.columns:\n",
    "        changed_col = col.strip().lower().replace(' ', '_').replace('-', '_')\n",
    "        df = df.withColumnRenamed(col, changed_col)\n",
    "    return df\n",
    "\n",
    "dem_df = trim_col_headers(dem_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dataset descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Statistical summary on data\n",
    "The descriptive statistics were shown in Step 1 already, so we will skip this step here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Missing values per dataset\n",
    "First we are measuring the amount of empty cells per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-043bd3ee": {
       "style": "primary"
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get no. of entries per column and divide by total rows\n",
    "imm_col_count = imm_full.describe().filter(F.col('summary') == 'count')\n",
    "air_col_count = air_df.describe().filter(F.col('summary') == 'count')\n",
    "dem_col_count = dem_df.describe().filter(F.col('summary') == 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print the numbers as percentage\n",
    "\n",
    "for df, numrows in zip([imm_col_count, air_col_count, dem_col_count], [imm_rows, air_rows, dem_rows]):\n",
    "    for column in df.columns:\n",
    "        if column == 'summary':\n",
    "            next\n",
    "        else:\n",
    "            df = df.withColumn(column, F.col(column).cast('int'))\n",
    "            df = df.withColumn(column, F.bround((F.col(column) / numrows * 100), scale=1))\n",
    "    print('{}'.format(df.show()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Unique values per column\n",
    "We can count the number of unique values per dataframe or column using `dropDuplicates()` and `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print the numbers as percentage\n",
    "for df, numrows in zip([imm_df, air_df, dem_df], [imm_rows, air_rows, dem_rows]):\n",
    "    for column in df.columns:\n",
    "        distinct_count = df.select(F.col(column)).distinct().count()\n",
    "        print('No. of unique entries in table \"{}\": {}'.format(column, distinc_count))\n",
    "    print('{}'.format(df.show()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Transformations\n",
    "This section will contain the steps required to transform the datasets.\n",
    "\n",
    "From previous notes we now carry out some transformation actions.\n",
    "1. Double to Integer (TFA-1)\n",
    "1. String to numbers (TFA-2)\n",
    "1. SAS Date Type to date (TFA-5)\n",
    "1. Non-Iso Date Type to date (TFA-6)\n",
    "\n",
    "Note that TFA-3 and TFA-4 were already carried out in the previous chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Integer Type Transformation (TFA-1)\n",
    "The fields mentioned here are IntegerType but were infered as double, we are changing them to integer.\n",
    "\n",
    "In addition the string column `entdepa` will also be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "int_fields = ['cicid','i94yr', 'i94mon', 'i94cit', 'i94res', 'arrdate', 'depdate', 'i94mode', 'i94bir','i94visa', 'count', 'biryear']\n",
    "\n",
    "def change_type(df=None, fields=None, totype=None):\n",
    "    if ((totype is not None) and (fields is not None)):\n",
    "        for column in fields:\n",
    "            cur_type = df.schema[column]\n",
    "            df = df.withColumn(column, F.col(column).cast(totype))\n",
    "            new_type = df.schema[column]\n",
    "            print('Done, switched column {} from {} to {}'.format(column, cur_type.dataType, new_type.dataType))\n",
    "        print(df.select(fields).show(5))\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "imm_df = change_type(imm_df, int_fields, 'int')\n",
    "imm_df = change_type(imm_df, ['admnum'], 'bigint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### String to numbers (TFA-2)\n",
    "In dataframes for demographic and airport data no float types were infered. Instead we see floating point numbers imported as strings.\n",
    "Those will be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Airport data:\n",
    "float_fields = ['elevation_ft']\n",
    "int_fields = None\n",
    "\n",
    "air_df = change_type(air_df, float_fields, 'double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Demographic Data\n",
    "float_fields = ['median_age', 'average_household_size', 'count' ]\n",
    "int_fields = ['male_population', 'female_population', 'total_population', 'number_of_veterans', 'foreign_born']\n",
    "\n",
    "dem_df = change_type(dem_df, float_fields, 'double')\n",
    "dem_df = change_type(dem_df, int_fields, 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### SAS Date Type Transformations (TFA-5)\n",
    "The SAS formatted dates (e.v. \"20573.0\") should be changed to proper date types.\n",
    "Note we are not putting this into the nb_helpers.py because it' s a more exemplary kind of transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_date_columns = ['arrdate', 'depdate']\n",
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "def sasdate_to_date(df=None, column_list=None):\n",
    "    # SAS has its own epoch, which we add temporarily\n",
    "    df = df.withColumn('epoch_start', F.lit(\"01-01-1960 00:00:00\"))\n",
    "    df = df.withColumn('epoch_start', F.to_date(F.col('epoch_start'), \"dd-M-yyyy\"))\n",
    "    # Then go through list of columns and convert double to int, then add this int to epoch_start\n",
    "    for column in column_list:\n",
    "        df = df.withColumn(column, F.col(column).cast('int'))\n",
    "        stm = 'date_add(epoch_start, {})'.format(column)\n",
    "        df = df.withColumn(column, F.expr(stm))\n",
    "    return df\n",
    "\n",
    "imm_df = sasdate_to_date(imm_df, sas_date_columns)\n",
    "imm_df.select(sas_date_columns).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Non-Iso Date Type Transformations (TFA-6)\n",
    "Columns `dtadtto` and `dtadtofile` contain non-iso dateformats and will be converted into proper date types as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# if required, update the dictionary with a new column and a new pattern or change existing\n",
    "imm_non_iso_dates = {'dtaddto': 'MMddyyyy', 'dtadfile': 'yyyyMMdd'}    \n",
    "imm_df = non_iso_date_change(imm_df, imm_non_iso_dates)\n",
    "\n",
    "print('Here a view on some examples: ')\n",
    "print(imm_df.select('dtaddto').show(5), imm_df.select('dtadfile').show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Missing values and nullable setting (TFA-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Changing nullable setting\n",
    "Changing columns in the i94 dataset to `nullable = true` where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define fields which should not be empty\n",
    "not_null_fields = ['cicid', 'admnum', 'i94yr', 'i94mon', 'i94cit', 'arrdate']\n",
    "imm_df = imm_df.dropna(subset=not_null_fields)\n",
    "imm_df = change_nullables(session=spark, df=imm_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Demographic file did not have any missing values, so we set all fields to nullable = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "not_null_fields = dem_df.columns\n",
    "dem_df = dem_df.na.drop(subset=not_null_fields)\n",
    "dem_df = change_nullables(session=spark, df=dem_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From airport data we want the items with iata code filled only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "not_null_fields = ['iata_code']\n",
    "air_df = air_df.na.drop(subset=not_null_fields)\n",
    "air_df = change_nullables(session=spark, df=air_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Writing schemas to JSON (optional: re-importing data with updated schema from JSON)\n",
    "Now that we have altered the dataframe schemas we export those schemas into JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write to Json file using a function\n",
    "def write_schema_json(df, schema_file):\n",
    "    import json\n",
    "    import sys\n",
    "    with open(schema_file, \"w\") as f:\n",
    "        json.dump(df.schema.jsonValue(), f)\n",
    "\n",
    "# Let's call that for each dataframe\n",
    "write_schema_json(imm_df, imm_schema_file)\n",
    "write_schema_json(dem_df, dem_schema_file)\n",
    "write_schema_json(air_df, air_schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "At this point one could read the JSON schema and apply it for importing data.\n",
    "\n",
    "However during my work I got a ClassCastException after importing the data on \".show()\" or any other actions, cancelling my error search here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save some lines by using a function to import a schema from JSON\n",
    "def import_schema(schema_file):\n",
    "    \"\"\" Function import_schema take a schema file as input.\n",
    "        The file should be JSON formatted and contain a valid Spark schema definition.\n",
    "        \n",
    "        Returns: Imported schema definition as Spark schema struct\"\"\"\n",
    "    \n",
    "    import json\n",
    "    schema_df = spark.sparkContext.wholeTextFiles(schema_file)\n",
    "    schema_content = schema_df.collect()[0][1]\n",
    "    schema_dict = json.loads(str(schema_content))\n",
    "    new_schema = T.StructType.fromJson(schema_dict)\n",
    "    return new_schema\n",
    "\n",
    "# Now apply the schema for each\n",
    "start_time = datetime.now()\n",
    "imm_df = spark.read.format('com.github.saurfang.sas.spark').load(path=imm_file, schema=import_schema(imm_schema_file))\n",
    "air_df = spark.read.csv(air_file, header=True, schema=import_schema(air_schema_file))\n",
    "dem_df = spark.read.csv(dem_file, header=True, sep=';', schema=import_schema(dem_schema_file))\n",
    "elapse = datetime.now() - start_time\n",
    "print('Reading ', imm_df.count(), ' lines from ', imm_file)\n",
    "print('Reading ', dem_df.count(), ' lines from ', dem_file)\n",
    "print('Reading ', air_df.count(), ' lines from ', air_file)\n",
    "print('Operation runtime ', elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Resulting schema definitions\n",
    "In the sections above we have transformed specific values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_df.printSchema()\n",
    "air_df.printSchema()\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Exporting the data to Parquet\n",
    "We export the collected data in Parquet format after we have processed the cleaning steps above.\n",
    "Note that in this notebook we are only writing a sample of the immigration data. Dataframe `imm_full` contains all 3M rows we will write in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_df.write.parquet(\"immigration_data\", mode='overwrite', compression='gzip')\n",
    "dem_df.write.parquet('demographic_data', mode='overwrite', compression='gzip')\n",
    "air_df.write.parquet('airport_data', mode='overwrite', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Logical exploration using PySpark\n",
    "In this chapter we will show some more explorative analysis that we help us understanding the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 01: Link demographic data to airport locations (city)\n",
    "The immigration data does not contain cities as full names but has a city code. Using the SAS definitions file we can add a list of cities as a dimension table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First get relevant rows from CSV-file, remove spaces and split column content with entries like <CITY,STATE> into a column for city name and a column for state\n",
    "                                         \n",
    "sas_codes = spark.read.csv('SAS-Value-Codes.csv', header=True, sep=';')\n",
    "sas_codes = sas_codes.filter(sas_codes.fieldname.contains('i94prt'))\n",
    "sas_codes = sas_codes.select(['short_code', 'content']).withColumn('content', F.lower(F.col('content')))\n",
    "sas_codes = sas_codes.withColumn('short_code', F.trim(F.col('short_code')))\n",
    "sas_codes = sas_codes.withColumn('city', F.split(sas_codes['content'], ',').getItem(0)).withColumn('state', F.split(sas_codes['content'], ',').getItem(1))\n",
    "sas_codes = sas_codes.withColumn('state', F.regexp_replace('state', '\\s', ''))\n",
    "sas_codes = sas_codes.drop('content')\n",
    "sas_codes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we check if we can match those to our airports dataframe using the short code in column `i94port`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20220"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get some fields from airports dataframe and 2 columns from imm_df\n",
    "airport_table = ['iata_code', 'name', 'municipality', 'iso_region', 'iso_country', 'coordinates']\n",
    "airport_codes = air_df.filter(air_df.iata_code.isNotNull()).select(airport_table)\n",
    "match_df = imm_df.select(['cicid', 'i94port'])\n",
    "\n",
    "# Get some fields from airports dataframe and 2 columns from imm_df\n",
    "airport_table = ['iata_code', 'name', 'municipality', 'iso_region', 'iso_country', 'coordinates']\n",
    "airport_codes = air_df.filter(air_df.iata_code.isNotNull()).select(airport_table)\n",
    "match_df = imm_df.select(['cicid', 'i94port'])\n",
    "\n",
    "# Match the data, the non-matches will receive \"null\"\n",
    "match_df = match_df.join(airport_codes, airport_codes.iata_code == sub_df.i94port, 'leftouter')\n",
    "nomatch = match_df.filter(F.col('i94port').isNull().count()\n",
    "match = match_df.filter(F.col('i94port').isNotNull().count()\n",
    "# Count number of matching lines\n",
    "print('Join failed for {} lines and matched for {} lines'.format(nomatch, match)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This catches **about 65% of the entries** in our fact table.\n",
    "\n",
    "Now we could also try to match the city name in the SAS values list to the airport`municipality` column, but we consider this even worse.\n",
    "\n",
    "Therefore we check the airport codes in the immigration table which are not matching to column `i94port`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`iata_code`' given input columns: [i94port];;\\n'Filter isnull('iata_code)\\n+- Project [i94port#6654]\\n   +- Project [cicid#6649, i94port#6654]\\n      +- LogicalRDD [cicid#6649, i94yr#6650, i94mon#6651, i94cit#6652, i94res#6653, i94port#6654, arrdate#6655, i94mode#6656, i94addr#6657, depdate#6658, i94bir#6659, i94visa#6660, count#6661, dtadfile#6662, visapost#6663, occup#6664, entdepa#6665, entdepd#6666, entdepu#6667, matflag#6668, biryear#6669, dtaddto#6670, gender#6671, insnum#6672, ... 5 more fields], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1768.filter.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`iata_code`' given input columns: [i94port];;\n'Filter isnull('iata_code)\n+- Project [i94port#6654]\n   +- Project [cicid#6649, i94port#6654]\n      +- LogicalRDD [cicid#6649, i94yr#6650, i94mon#6651, i94cit#6652, i94res#6653, i94port#6654, arrdate#6655, i94mode#6656, i94addr#6657, depdate#6658, i94bir#6659, i94visa#6660, count#6661, dtadfile#6662, visapost#6663, occup#6664, entdepa#6665, entdepd#6666, entdepu#6667, matflag#6668, biryear#6669, dtaddto#6670, gender#6671, insnum#6672, ... 5 more fields], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3411)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1484)\n\tat sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-708e819d08ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get distinct entries which are missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnot_matching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i94port'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iata_code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshort_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnot_matching\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i94port'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshort_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msas_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msas_codes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshort_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi94port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'leftouter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`iata_code`' given input columns: [i94port];;\\n'Filter isnull('iata_code)\\n+- Project [i94port#6654]\\n   +- Project [cicid#6649, i94port#6654]\\n      +- LogicalRDD [cicid#6649, i94yr#6650, i94mon#6651, i94cit#6652, i94res#6653, i94port#6654, arrdate#6655, i94mode#6656, i94addr#6657, depdate#6658, i94bir#6659, i94visa#6660, count#6661, dtadfile#6662, visapost#6663, occup#6664, entdepa#6665, entdepd#6666, entdepu#6667, matflag#6668, biryear#6669, dtaddto#6670, gender#6671, insnum#6672, ... 5 more fields], false\\n\""
     ]
    }
   ],
   "source": [
    "# Get list of entries which are missing\n",
    "not_matching = match_df.select(['i94port']).filter(F.col('iata_code').isNull())\n",
    "short_list = not_matching.select(['i94port']).distinct()\n",
    "short_list.join(sas_codes, sas_codes.short_code == short_list.i94port, 'leftouter').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We could improve data quality by updating the airport code table with those missing codes.\n",
    "This is how many rows can be added to the fact table per entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|i94port|count|\n",
      "+-------+-----+\n",
      "|    JKM|    1|\n",
      "|    MOR|    1|\n",
      "|    PHR|    3|\n",
      "|    MIL|    4|\n",
      "|    YHC|    6|\n",
      "|    INP|    7|\n",
      "|    PHU|   13|\n",
      "|    YGF|   24|\n",
      "|    X96|   29|\n",
      "|    NOL|   30|\n",
      "|    XXX|   33|\n",
      "|    SAJ|  104|\n",
      "|    SAI|  226|\n",
      "|    WAS|  761|\n",
      "|    LVG|  888|\n",
      "|    FTL|  965|\n",
      "|    CHI| 1323|\n",
      "|    HHW| 1403|\n",
      "|    NYC| 4888|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_airport_codes.groupBy('i94port').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Logical exploration example 2: Filter airport data to get a dimension table (TFA-8)\n",
    "The airport codes table has over 50k entries. We can reduce it by filtering for *type airport* and for matching with immigration data we also need the *iata_code*.\n",
    "\n",
    "The resulting dataframe from the airport dataset will be kept as a transformed version of the table (trimmed of spaces in column `iata_code` and exported to Parquet later (dimension table \"airport\").\n",
    "\n",
    "The code below creates five examples of travellers and their corresponding airport information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+----------+--------------------+------------+----------+-----------+--------------------+\n",
      "|airport_code|  cicid|   arrdate|                name|municipality|iso_region|iso_country|         coordinates|\n",
      "+------------+-------+----------+--------------------+------------+----------+-----------+--------------------+\n",
      "|         BGM| 154968|2016-04-01|Greater Binghamto...|  Binghamton|     US-NY|         US|-75.97979736, 42....|\n",
      "|         FMY|3147212|2016-04-17|          Page Field|  Fort Myers|     US-FL|         US|-81.8632965087999...|\n",
      "|         FMY|3366926|2016-04-18|          Page Field|  Fort Myers|     US-FL|         US|-81.8632965087999...|\n",
      "|         FMY|3148262|2016-04-17|          Page Field|  Fort Myers|     US-FL|         US|-81.8632965087999...|\n",
      "|         FMY|5200043|2016-04-27|          Page Field|  Fort Myers|     US-FL|         US|-81.8632965087999...|\n",
      "+------------+-------+----------+--------------------+------------+----------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all airport codes from the airport dataframe\n",
    "airport_table = ['iata_code', 'name', 'municipality', 'iso_region', 'iso_country', 'coordinates']\n",
    "airport_codes = air_df.filter(air_df.iata_code.isNotNull()).select(airport_table).withColumnRenamed('iata_code', 'airport_code')\n",
    "\n",
    "# Read Columns from immigration dataset\n",
    "imm_airports = imm_df.select(['cicid', 'arrdate', 'i94port']).withColumnRenamed('i94port', 'airport_code').distinct()\n",
    "\n",
    "# Clean the codes from any spaces\n",
    "airport_code = airport_codes.withColumn('airport_code', F.trim(F.col('airport_code')))\n",
    "imm_airports = imm_airports.withColumn('airport_code', F.trim(F.col('airport_code')))\n",
    "\n",
    "# Join with immigration dataset\n",
    "imm_airports.join(airport_codes, on='airport_code').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Logical exploration example 3: Renaming columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(imm_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
