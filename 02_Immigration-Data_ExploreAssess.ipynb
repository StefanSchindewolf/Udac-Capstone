{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Data ETL\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2 Explore and assess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Overview\n",
    "In this section we will explore the data, idenfify and implement necessary transformations and finally export the data to S3 in parquet format.\n",
    "\n",
    "Subsections of this Notebook:\n",
    "* Preparations (imports, file locations, etc.)\n",
    "* Dataframe schema analysis\n",
    "* Explorative analysis of the data\n",
    "* Dataframe transformation steps\n",
    "* Defining a new schema\n",
    "* Writing data to S3 in Parquet format\n",
    "* Examples of analyzing table contents using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from os.path import getsize\n",
    "from nb_helpers import summarize_data, get_sas_definitions, read_sas_in_chunks, read_csv_print, print_stat, non_iso_date_change, change_nullables\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s %(levelname)s \\t %(message)s ',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "log = logging.getLogger('log')\n",
    "\n",
    "# Improve view\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Reading data files into Spark Dataframes\n",
    "The next cell contains the file locations for each dataset, you may adjust it if required. Also note the configuration of the sample size which will improve performance while making some basic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Source file locations\n",
    "dem_file = 'us-cities-demographics.csv'\n",
    "imm_file = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "air_file = 'airport-codes_csv.csv'\n",
    "\n",
    "# Schema file locations\n",
    "air_schema_file = 'air_schema.json'\n",
    "dem_schema_file = 'dem_schema.json'\n",
    "imm_schema_file = 'imm_schema.json'\n",
    "\n",
    "\n",
    "# If working on a sample of the large immigration dataset, set your sample size here:\n",
    "sample_size = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "During my analysis I changed the schema of each dataset imported and based on the transformations done created a new schema definition. The definition is stored in a JSON file.\n",
    "\n",
    "The data will be **imported again** using those schema **at the end** of the \"Explore and Assess\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we setup the Spark Session and read all three files into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  3096313  lines from  ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "Reading  2891  lines from  us-cities-demographics.csv\n",
      "Reading  55075  lines from  airport-codes_csv.csv\n",
      "Operation runtime  0:00:45.826218\n"
     ]
    }
   ],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "############# TEST option(\"inferSchema\", \"true\")\n",
    "start_time = datetime.now()\n",
    "imm_df = spark.read.format('com.github.saurfang.sas.spark').option(\"inferSchema\",\"true\").load(imm_file)\n",
    "imm_rows = imm_df.count()\n",
    "air_df = spark.read.csv(air_file, header=True)\n",
    "air_rows = air_df.count()\n",
    "dem_df = spark.read.csv(dem_file, header=True)\n",
    "dem_rows = dem_df.count()\n",
    "df_list = {'imm_df': imm_df, 'air_df': air_df, 'dem_df': dem_df}\n",
    "imm_full = imm_df\n",
    "elapse = datetime.now() - start_time\n",
    "print('Reading ', imm_rows , ' lines from ', imm_file)\n",
    "print('Reading ', dem_rows, ' lines from ', dem_file)\n",
    "print('Reading ', air_rows, ' lines from ', air_file)\n",
    "print('Operation runtime ', elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Explore and Describe data\n",
    "This section contains the following subsections:\n",
    "* Spark schema analysis\n",
    "* Spark dataframe descriptions analysis\n",
    "\n",
    "Before we start this section we are naming each dataframe and reducing immigration data to a sample of the complete dataframe, keeping the full set in variable `imm_full` (the sample size being configured at the beginning of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_df = imm_full.sample(sample_size)\n",
    "imm_df.name = 'i94 immigration data'\n",
    "dem_df.name = 'city demographics'\n",
    "air_df.name = 'airport codes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note:**\n",
    "\n",
    "    * We want to keep track of transformation steps\n",
    "    * So if a required transformation is identified, it will be assigned a number like \"TFA-x\"\n",
    "    * Where \"TFA\" stands for transformation action (plus a number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Display schemas created automatically by Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imm_df.printSchema()\n",
    "air_df.printSchema()\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note** (TFA-1) Several columns in the i94 dataframe were infered as double while they should be of data type integer. Those types should be changed (see `cicid, i94yr,...`).\n",
    "\n",
    "**Note** (TFA-2) Several columns in airport and demographic data were infered as string where they should be either double or integer (see airport data, `elevation_ft` or demographic data `median_age`\n",
    "\n",
    "**Note** (TFA-3) Demographics data file was imported as a one-column file due to ';' being used as delimiter. We read the file again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dem_df = spark.read.csv(dem_file, header=True, sep=';')\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note**: (TFA-4) For easier handling we strip columns of spaces, apply lower case and replace inner spaces and '-' with underscore '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def trim_col_headers(df):\n",
    "    for col in df.columns:\n",
    "        changed_col = col.strip().lower().replace(' ', '_').replace('-', '_')\n",
    "        df = df.withColumnRenamed(col, changed_col)\n",
    "    return df\n",
    "\n",
    "dem_df = trim_col_headers(dem_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dataset descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Statistical summary on data\n",
    "The descriptive statistics were shown in Step 1 already, so we will skip this step here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Missing values per dataset\n",
    "First we are measuring the amount of empty cells per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get no. of entries per column and divide by total rows\n",
    "imm_col_count = imm_full.describe().filter(F.col('summary') == 'count')\n",
    "air_col_count = air_df.describe().filter(F.col('summary') == 'count')\n",
    "dem_col_count = dem_df.describe().filter(F.col('summary') == 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|summary|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|\n",
      "+-------+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|  count|100.0|100.0| 100.0| 100.0| 100.0|  100.0|  100.0|  100.0|   95.1|   95.4| 100.0|  100.0|100.0|   100.0|    39.2|  0.3|  100.0|   95.5|    0.0|   95.5|  100.0|  100.0|  86.6|   3.7|   97.3| 100.0| 99.4|   100.0|\n",
      "+-------+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "\n",
      "None\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|summary|ident| type| name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|  count|100.0|100.0|100.0|        87.3|    100.0|      100.0|     100.0|        89.7|    74.5|     16.7|      52.1|      100.0|\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n",
      "None\n",
      "+-------+-----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----+-----+\n",
      "|summary| city|state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code| race|count|\n",
      "+-------+-----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----+-----+\n",
      "|  count|100.0|100.0|     100.0|           99.9|             99.9|           100.0|              99.6|        99.6|                  99.4|     100.0|100.0|100.0|\n",
      "+-------+-----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the numbers as percentage\n",
    "\n",
    "for df, numrows in zip([imm_col_count, air_col_count, dem_col_count], [imm_rows, air_rows, dem_rows]):\n",
    "    for column in df.columns:\n",
    "        if column == 'summary':\n",
    "            next\n",
    "        else:\n",
    "            df = df.withColumn(column, F.col(column).cast('int'))\n",
    "            df = df.withColumn(column, F.bround((F.col(column) / numrows * 100), scale=1))\n",
    "    print('{}'.format(df.show()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Unique values per column\n",
    "We can count the number of unique values per dataframe or column using `dropDuplicates()` and `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of duplicates in table \"cicid\": 30666\n",
      "No. of duplicates in table \"i94yr\": 1\n",
      "No. of duplicates in table \"i94mon\": 1\n",
      "No. of duplicates in table \"i94cit\": 177\n",
      "No. of duplicates in table \"i94res\": 181\n",
      "No. of duplicates in table \"i94port\": 152\n",
      "No. of duplicates in table \"arrdate\": 30\n",
      "No. of duplicates in table \"i94mode\": 5\n",
      "No. of duplicates in table \"i94addr\": 100\n",
      "No. of duplicates in table \"depdate\": 172\n",
      "No. of duplicates in table \"i94bir\": 97\n",
      "No. of duplicates in table \"i94visa\": 3\n",
      "No. of duplicates in table \"count\": 1\n",
      "No. of duplicates in table \"dtadfile\": 61\n",
      "No. of duplicates in table \"visapost\": 251\n",
      "No. of duplicates in table \"occup\": 22\n",
      "No. of duplicates in table \"entdepa\": 11\n",
      "No. of duplicates in table \"entdepd\": 12\n",
      "No. of duplicates in table \"entdepu\": 2\n",
      "No. of duplicates in table \"matflag\": 2\n",
      "No. of duplicates in table \"biryear\": 97\n",
      "No. of duplicates in table \"dtaddto\": 256\n",
      "No. of duplicates in table \"gender\": 5\n",
      "No. of duplicates in table \"insnum\": 214\n",
      "No. of duplicates in table \"airline\": 171\n",
      "No. of duplicates in table \"admnum\": 30665\n",
      "No. of duplicates in table \"fltno\": 2015\n",
      "No. of duplicates in table \"visatype\": 15\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| 107.0|2016.0|   4.0| 103.0| 103.0|    DET|20545.0|    1.0|     MI|20556.0|  33.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1983.0|06292016|     M|  null|     LH|5.5424716533E10|00442|      WT|\n",
      "| 149.0|2016.0|   4.0| 103.0| 103.0|    NEW|20545.0|    1.0|     NY|20552.0|  34.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1982.0|06292016|     F|  null|     LH|5.5440588633E10|00402|      WT|\n",
      "| 200.0|2016.0|   4.0| 103.0| 103.0|    HOU|20545.0|    1.0|     TX|20635.0|  41.0|    1.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1975.0|06292016|     M|  null|     LH|5.5432435833E10|00440|      WB|\n",
      "| 228.0|2016.0|   4.0| 103.0| 103.0|    NYC|20545.0|    1.0|     NY|20550.0|  39.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1977.0|06292016|     F|  null|     OS|5.5436868833E10|00087|      WT|\n",
      "| 397.0|2016.0|   4.0| 103.0| 103.0|    MIA|20545.0|    1.0|     FL|20561.0|  72.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1944.0|06292016|     F|  null|     OS|5.5434850233E10|00097|      WT|\n",
      "| 574.0|2016.0|   4.0| 103.0| 103.0|    SFR|20545.0|    1.0|     CA|20567.0|  64.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1952.0|06292016|     F|  null|     LH|5.5434037133E10|00454|      WT|\n",
      "| 610.0|2016.0|   4.0| 103.0| 103.0|    LVG|20545.0|    1.0|     NV|20546.0|  58.0|    2.0|  1.0|20160401|     BRL| null|      G|      R|   null|      M| 1958.0|09302016|     M|  null|    *GA| 9.246114363E10|VPCSW|      B2|\n",
      "| 644.0|2016.0|   4.0| 103.0| 103.0|    LOS|20545.0|    1.0|     CA|20698.0|  53.0|    3.0|  1.0|20160401|     VNN| null|      G|      O|   null|      M| 1963.0|     D/S|     F|  null|     LH| 9.248134413E10|00456|      F1|\n",
      "| 868.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     FL|20549.0|  46.0|    2.0|  1.0|20160401|    null| null|      G|      I|   null|      M| 1970.0|06292016|     F|  null|     LH|5.5437888033E10|00402|      WT|\n",
      "| 935.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     NY|20549.0|  51.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1965.0|06292016|     F|  null|     UA|5.5442925133E10|00055|      WT|\n",
      "| 952.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     NY|20549.0|  52.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1964.0|06292016|     M|  null|     DL|5.5447016233E10|00149|      WT|\n",
      "|1025.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     NY|20551.0|  47.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1969.0|06292016|     F|  null|     UA|5.5443908833E10|00055|      WT|\n",
      "|1096.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     NY|20553.0|  46.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1970.0|06292016|     F|  null|     UA|5.5435796233E10|02067|      WT|\n",
      "|1334.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20550.0|  57.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1959.0|06292016|     F|  null|     SN|5.5421062733E10|01401|      WT|\n",
      "|1336.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20550.0|  55.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1961.0|06292016|     M|  null|     BA|5.5449052133E10|00115|      WT|\n",
      "|1393.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20550.0|  15.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 2001.0|06292016|     F|  null|     AB|5.5421388033E10|07450|      WT|\n",
      "|1495.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20552.0|  27.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1989.0|06292016|     F|  null|     SN|5.5421360633E10|01401|      WT|\n",
      "|1666.0|2016.0|   4.0| 104.0| 104.0|    MIA|20545.0|    1.0|     FL|20561.0|  58.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1958.0|06292016|     F|  null|     AA|5.5458336133E10|00039|      WT|\n",
      "|1685.0|2016.0|   4.0| 104.0| 104.0|    MIA|20545.0|    1.0|     FL|20546.0|  62.0|    2.0|  1.0|20160401|    null| null|      G|      R|   null|      M| 1954.0|06292016|     F|  null|     LX|5.5425672633E10|00066|      WT|\n",
      "|1946.0|2016.0|   4.0| 104.0| 121.0|    WAS|20545.0|    1.0|     MD|20555.0|  43.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1973.0|06292016|     F|  null|     KL|5.5440882933E10|00651|      WT|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "No. of duplicates in table \"ident\": 55075\n",
      "No. of duplicates in table \"type\": 7\n",
      "No. of duplicates in table \"name\": 52144\n",
      "No. of duplicates in table \"elevation_ft\": 5450\n",
      "No. of duplicates in table \"continent\": 7\n",
      "No. of duplicates in table \"iso_country\": 244\n",
      "No. of duplicates in table \"iso_region\": 2810\n",
      "No. of duplicates in table \"municipality\": 27134\n",
      "No. of duplicates in table \"gps_code\": 40851\n",
      "No. of duplicates in table \"iata_code\": 9043\n",
      "No. of duplicates in table \"local_code\": 27437\n",
      "No. of duplicates in table \"coordinates\": 54874\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "| 00AS|small_airport|      Fulton Airport|        1100|       NA|         US|     US-OK|        Alex|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      Cordes Airport|        3810|       NA|         US|     US-AZ|      Cordes|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|        3038|       NA|         US|     US-CA|     Barstow|    00CA|     null|      00CA|-116.888000488, 3...|\n",
      "| 00CL|small_airport| Williams Ag Airport|          87|       NA|         US|     US-CA|       Biggs|    00CL|     null|      00CL|-121.763427, 39.4...|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|        3350|       NA|         US|     US-CA| Pine Valley|    00CN|     null|      00CN|-116.4597417, 32....|\n",
      "| 00CO|       closed|          Cass Field|        4830|       NA|         US|     US-CO|  Briggsdale|    null|     null|      null|-104.344002, 40.6...|\n",
      "| 00FA|small_airport| Grass Patch Airport|          53|       NA|         US|     US-FL|    Bushnell|    00FA|     null|      00FA|-82.2190017700195...|\n",
      "| 00FD|     heliport|  Ringhaver Heliport|          25|       NA|         US|     US-FL|   Riverview|    00FD|     null|      00FD|-82.3453979492187...|\n",
      "| 00FL|small_airport|   River Oak Airport|          35|       NA|         US|     US-FL|  Okeechobee|    00FL|     null|      00FL|-80.9692001342773...|\n",
      "| 00GA|small_airport|    Lt World Airport|         700|       NA|         US|     US-GA|    Lithonia|    00GA|     null|      00GA|-84.0682983398437...|\n",
      "| 00GE|     heliport|    Caffrey Heliport|         957|       NA|         US|     US-GA|       Hiram|    00GE|     null|      00GE|-84.7339019775390...|\n",
      "| 00HI|     heliport|  Kaupulehu Heliport|          43|       NA|         US|     US-HI| Kailua/Kona|    00HI|     null|      00HI|-155.980233, 19.8...|\n",
      "| 00ID|small_airport|Delta Shores Airport|        2064|       NA|         US|     US-ID|  Clark Fork|    00ID|     null|      00ID|-116.213996887207...|\n",
      "| 00IG|small_airport|       Goltl Airport|        3359|       NA|         US|     US-KS|    McDonald|    00IG|     null|      00IG|-101.395994, 39.7...|\n",
      "| 00II|     heliport|Bailey Generation...|         600|       NA|         US|     US-IN|  Chesterton|    00II|     null|      00II|-87.122802734375,...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "No. of duplicates in table \"city\": 567\n",
      "No. of duplicates in table \"state\": 49\n",
      "No. of duplicates in table \"median_age\": 180\n",
      "No. of duplicates in table \"male_population\": 594\n",
      "No. of duplicates in table \"female_population\": 595\n",
      "No. of duplicates in table \"total_population\": 594\n",
      "No. of duplicates in table \"number_of_veterans\": 578\n",
      "No. of duplicates in table \"foreign_born\": 588\n",
      "No. of duplicates in table \"average_household_size\": 162\n",
      "No. of duplicates in table \"state_code\": 49\n",
      "No. of duplicates in table \"race\": 5\n",
      "No. of duplicates in table \"count\": 2785\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|            city|         state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|                race| count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White| 58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...| 24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White| 76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...|  1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...| 11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian| 32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian| 11060|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|American Indian a...|   998|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|         741270|           826172|         1567442|             61995|      205339|                  2.61|        PA|               Asian|122721|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...|  8791|\n",
      "|      Fort Myers|       Florida|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|               White| 50169|\n",
      "|      Pittsburgh|  Pennsylvania|      32.9|         149690|           154695|          304385|             17728|       28187|                  2.13|        PA|               White|208863|\n",
      "|          Laredo|         Texas|      28.8|         124305|           131484|          255789|              4921|       68427|                  3.66|        TX|American Indian a...|  1253|\n",
      "|        Berkeley|    California|      32.5|          60142|            60829|          120971|              3736|       25000|                  2.35|        CA|               Asian| 27089|\n",
      "|     Santa Clara|    California|      35.2|          63278|            62938|          126216|              4426|       52281|                  2.75|        CA|               White| 55847|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the numbers as percentage\n",
    "for df, numrows in zip([imm_df, air_df, dem_df], [imm_rows, air_rows, dem_rows]):\n",
    "    for column in df.columns:\n",
    "        distinct_count = df.select(F.col(column)).distinct().count()\n",
    "        print('No. of unique entries in table \"{}\": {}'.format(column, (dup_count))\n",
    "    print('{}'.format(df.show()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Transformations\n",
    "This section will contain the steps required to transform the datasets.\n",
    "\n",
    "From previous notes we now carry out some transformation actions.\n",
    "1. Double to Integer (TFA-1)\n",
    "1. String to numbers (TFA-2)\n",
    "1. SAS Date Type to date (TFA-5)\n",
    "1. Non-Iso Date Type to date (TFA-6)\n",
    "\n",
    "Note that TFA-3 and TFA-4 were already carried out in the previous chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Integer Type Transformation (TFA-1)\n",
    "The fields mentioned here are IntegerType but were infered as double, we are changing them to integer.\n",
    "\n",
    "In addition the string column `entdepa` will also be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, switched column cicid from DoubleType to IntegerType\n",
      "Done, switched column i94yr from DoubleType to IntegerType\n",
      "Done, switched column i94mon from DoubleType to IntegerType\n",
      "Done, switched column i94cit from DoubleType to IntegerType\n",
      "Done, switched column i94res from DoubleType to IntegerType\n",
      "Done, switched column arrdate from DoubleType to IntegerType\n",
      "Done, switched column depdate from DoubleType to IntegerType\n",
      "Done, switched column i94mode from DoubleType to IntegerType\n",
      "Done, switched column i94bir from DoubleType to IntegerType\n",
      "Done, switched column i94visa from DoubleType to IntegerType\n",
      "Done, switched column count from DoubleType to IntegerType\n",
      "Done, switched column biryear from DoubleType to IntegerType\n",
      "+-----+-----+------+------+------+-------+-------+-------+------+-------+-----+-------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|arrdate|depdate|i94mode|i94bir|i94visa|count|biryear|\n",
      "+-----+-----+------+------+------+-------+-------+-------+------+-------+-----+-------+\n",
      "|  107| 2016|     4|   103|   103|  20545|  20556|      1|    33|      2|    1|   1983|\n",
      "|  149| 2016|     4|   103|   103|  20545|  20552|      1|    34|      2|    1|   1982|\n",
      "|  200| 2016|     4|   103|   103|  20545|  20635|      1|    41|      1|    1|   1975|\n",
      "|  228| 2016|     4|   103|   103|  20545|  20550|      1|    39|      2|    1|   1977|\n",
      "|  397| 2016|     4|   103|   103|  20545|  20561|      1|    72|      2|    1|   1944|\n",
      "+-----+-----+------+------+------+-------+-------+-------+------+-------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Done, switched column admnum from DoubleType to LongType\n",
      "+-----------+\n",
      "|     admnum|\n",
      "+-----------+\n",
      "|55424716533|\n",
      "|55440588633|\n",
      "|55432435833|\n",
      "|55436868833|\n",
      "|55434850233|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "int_fields = ['cicid','i94yr', 'i94mon', 'i94cit', 'i94res', 'arrdate', 'depdate', 'i94mode', 'i94bir','i94visa', 'count', 'biryear']\n",
    "\n",
    "def change_type(df=None, fields=None, totype=None):\n",
    "    if ((totype is not None) and (fields is not None)):\n",
    "        for column in fields:\n",
    "            cur_type = df.schema[column]\n",
    "            df = df.withColumn(column, F.col(column).cast(totype))\n",
    "            new_type = df.schema[column]\n",
    "            print('Done, switched column {} from {} to {}'.format(column, cur_type.dataType, new_type.dataType))\n",
    "        print(df.select(fields).show(5))\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "imm_df = change_type(imm_df, int_fields, 'int')\n",
    "imm_df = change_type(imm_df, ['admnum'], 'bigint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### String to numbers (TFA-2)\n",
    "In dataframes for demographic and airport data no float types were infered. Instead we see floating point numbers imported as strings.\n",
    "Those will be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, switched column elevation_ft from StringType to DoubleType\n",
      "+------------+\n",
      "|elevation_ft|\n",
      "+------------+\n",
      "|        11.0|\n",
      "|      3435.0|\n",
      "|       450.0|\n",
      "|       820.0|\n",
      "|       237.0|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Airport data:\n",
    "float_fields = ['elevation_ft']\n",
    "int_fields = None\n",
    "\n",
    "air_df = change_type(air_df, float_fields, 'double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, switched column median_age from StringType to DoubleType\n",
      "Done, switched column average_household_size from StringType to DoubleType\n",
      "Done, switched column count from StringType to DoubleType\n",
      "+----------+----------------------+-------+\n",
      "|median_age|average_household_size|  count|\n",
      "+----------+----------------------+-------+\n",
      "|      33.8|                   2.6|25924.0|\n",
      "|      41.0|                  2.39|58723.0|\n",
      "|      38.5|                  2.58| 4759.0|\n",
      "|      34.5|                  3.18|24437.0|\n",
      "|      34.6|                  2.73|76402.0|\n",
      "+----------+----------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Done, switched column male_population from StringType to IntegerType\n",
      "Done, switched column female_population from StringType to IntegerType\n",
      "Done, switched column total_population from StringType to IntegerType\n",
      "Done, switched column number_of_veterans from StringType to IntegerType\n",
      "Done, switched column foreign_born from StringType to IntegerType\n",
      "+---------------+-----------------+----------------+------------------+------------+\n",
      "|male_population|female_population|total_population|number_of_veterans|foreign_born|\n",
      "+---------------+-----------------+----------------+------------------+------------+\n",
      "|          40601|            41862|           82463|              1562|       30908|\n",
      "|          44129|            49500|           93629|              4147|       32935|\n",
      "|          38040|            46799|           84839|              4819|        8229|\n",
      "|          88127|            87105|          175232|              5821|       33878|\n",
      "|         138040|           143873|          281913|              5829|       86253|\n",
      "+---------------+-----------------+----------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Demographic Data\n",
    "float_fields = ['median_age', 'average_household_size', 'count' ]\n",
    "int_fields = ['male_population', 'female_population', 'total_population', 'number_of_veterans', 'foreign_born']\n",
    "\n",
    "dem_df = change_type(dem_df, float_fields, 'double')\n",
    "dem_df = change_type(dem_df, int_fields, 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### SAS Date Type Transformations (TFA-5)\n",
    "The SAS formatted dates (e.v. \"20573.0\") should be changed to proper date types.\n",
    "Note we are not putting this into the nb_helpers.py because it' s a more exemplary kind of transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|   arrdate|   depdate|\n",
      "+----------+----------+\n",
      "|2016-04-01|2016-04-12|\n",
      "|2016-04-01|2016-04-08|\n",
      "|2016-04-01|2016-06-30|\n",
      "|2016-04-01|2016-04-06|\n",
      "|2016-04-01|2016-04-17|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sas_date_columns = ['arrdate', 'depdate']\n",
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "def sasdate_to_date(df=None, column_list=None):\n",
    "    # SAS has its own epoch, which we add temporarily\n",
    "    df = df.withColumn('epoch_start', F.lit(\"01-01-1960 00:00:00\"))\n",
    "    df = df.withColumn('epoch_start', F.to_date(F.col('epoch_start'), \"dd-M-yyyy\"))\n",
    "    # Then go through list of columns and convert double to int, then add this int to epoch_start\n",
    "    for column in column_list:\n",
    "        df = df.withColumn(column, F.col(column).cast('int'))\n",
    "        stm = 'date_add(epoch_start, {})'.format(column)\n",
    "        df = df.withColumn(column, F.expr(stm))\n",
    "    return df\n",
    "\n",
    "imm_df = sasdate_to_date(imm_df, sas_date_columns)\n",
    "imm_df.select(sas_date_columns).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Non-Iso Date Type Transformations (TFA-6)\n",
    "Columns `dtadtto` and `dtadtofile` contain non-iso dateformats and will be converted into proper date types as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Changing SAS Date Type to Spark Datetype\n",
      "Done, switched column dtaddto from StringType to DateType\n",
      "Done, switched column dtadfile from StringType to DateType\n",
      "Here a view on some examples: \n",
      "+----------+\n",
      "|   dtaddto|\n",
      "+----------+\n",
      "|2016-06-29|\n",
      "|2016-06-29|\n",
      "|2016-06-29|\n",
      "|2016-06-29|\n",
      "|2016-06-29|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|  dtadfile|\n",
      "+----------+\n",
      "|2016-04-01|\n",
      "|2016-04-01|\n",
      "|2016-04-01|\n",
      "|2016-04-01|\n",
      "|2016-04-01|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "# if required, update the dictionary with a new column and a new pattern or change existing\n",
    "imm_non_iso_dates = {'dtaddto': 'MMddyyyy', 'dtadfile': 'yyyyMMdd'}    \n",
    "imm_df = non_iso_date_change(imm_df, imm_non_iso_dates)\n",
    "\n",
    "print('Here a view on some examples: ')\n",
    "print(imm_df.select('dtaddto').show(5), imm_df.select('dtadfile').show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Missing values and nullable setting (TFA-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Changing nullable setting\n",
    "Changing columns in the i94 dataset to `nullable = true` where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, Schema changed for  6  columns.\n"
     ]
    }
   ],
   "source": [
    "# Define fields which should not be empty\n",
    "not_null_fields = ['cicid', 'admnum', 'i94yr', 'i94mon', 'i94cit', 'arrdate']\n",
    "imm_df = imm_df.dropna(subset=not_null_fields)\n",
    "imm_df = change_nullables(session=spark, df=imm_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Demographic file did not have any missing values, so we set all fields to nullable = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, Schema changed for  12  columns.\n"
     ]
    }
   ],
   "source": [
    "not_null_fields = dem_df.columns\n",
    "dem_df = dem_df.na.drop(subset=not_null_fields)\n",
    "dem_df = change_nullables(session=spark, df=dem_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From airport data we want the items with iata code filled only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, Schema changed for  1  columns.\n"
     ]
    }
   ],
   "source": [
    "not_null_fields = ['iata_code']\n",
    "air_df = air_df.na.drop(subset=not_null_fields)\n",
    "air_df = change_nullables(session=spark, df=air_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Writing schemas to JSON (optional: re-importing data with updated schema from JSON)\n",
    "Now that we have altered the dataframe schemas we export those schemas into JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write to Json file using a function\n",
    "def write_schema_json(df, schema_file):\n",
    "    import json\n",
    "    import sys\n",
    "    with open(schema_file, \"w\") as f:\n",
    "        json.dump(df.schema.jsonValue(), f)\n",
    "\n",
    "# Let's call that for each dataframe\n",
    "write_schema_json(imm_df, imm_schema_file)\n",
    "write_schema_json(dem_df, dem_schema_file)\n",
    "write_schema_json(air_df, air_schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "At this point one could read the JSON schema and apply it for importing data.\n",
    "\n",
    "However during my work I got a ClassCastException after importing the data on \".show()\" or any other actions, cancelling my error search here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save some lines by using a function to import a schema from JSON\n",
    "def import_schema(schema_file):\n",
    "    \"\"\" Function import_schema take a schema file as input.\n",
    "        The file should be JSON formatted and contain a valid Spark schema definition.\n",
    "        \n",
    "        Returns: Imported schema definition as Spark schema struct\"\"\"\n",
    "    \n",
    "    import json\n",
    "    schema_df = spark.sparkContext.wholeTextFiles(schema_file)\n",
    "    schema_content = schema_df.collect()[0][1]\n",
    "    schema_dict = json.loads(str(schema_content))\n",
    "    new_schema = T.StructType.fromJson(schema_dict)\n",
    "    return new_schema\n",
    "\n",
    "# Now apply the schema for each\n",
    "start_time = datetime.now()\n",
    "imm_df = spark.read.format('com.github.saurfang.sas.spark').load(path=imm_file, schema=import_schema(imm_schema_file))\n",
    "air_df = spark.read.csv(air_file, header=True, schema=import_schema(air_schema_file))\n",
    "dem_df = spark.read.csv(dem_file, header=True, sep=';', schema=import_schema(dem_schema_file))\n",
    "elapse = datetime.now() - start_time\n",
    "print('Reading ', imm_df.count(), ' lines from ', imm_file)\n",
    "print('Reading ', dem_df.count(), ' lines from ', dem_file)\n",
    "print('Reading ', air_df.count(), ' lines from ', air_file)\n",
    "print('Operation runtime ', elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Resulting schema definitions\n",
    "In the sections above we have transformed specific values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = false)\n",
      " |-- i94yr: integer (nullable = false)\n",
      " |-- i94mon: integer (nullable = false)\n",
      " |-- i94cit: integer (nullable = false)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: date (nullable = false)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: date (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- dtadfile: date (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: date (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: long (nullable = false)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- epoch_start: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: double (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = false)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = false)\n",
      " |-- state: string (nullable = false)\n",
      " |-- median_age: double (nullable = false)\n",
      " |-- male_population: integer (nullable = false)\n",
      " |-- female_population: integer (nullable = false)\n",
      " |-- total_population: integer (nullable = false)\n",
      " |-- number_of_veterans: integer (nullable = false)\n",
      " |-- foreign_born: integer (nullable = false)\n",
      " |-- average_household_size: double (nullable = false)\n",
      " |-- state_code: string (nullable = false)\n",
      " |-- race: string (nullable = false)\n",
      " |-- count: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imm_df.printSchema()\n",
    "air_df.printSchema()\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Exporting the data to Parquet\n",
    "We export the collected data in Parquet format after we have processed the cleaning steps above.\n",
    "Note that in this notebook we are only writing a sample of the immigration data. Dataframe `imm_full` contains all 3M rows we will write in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_df.write.parquet(\"immigration_data\", mode='overwrite', compression='gzip')\n",
    "dem_df.write.parquet('demographic_data', mode='overwrite', compression='gzip')\n",
    "air_df.write.parquet('airport_data', mode='overwrite', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Logical exploration using PySpark\n",
    "In this chapter we will show some more explorative analysis that we help us understanding the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 01: Link demographic data with airport/city\n",
    "Select the valid median age per airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_df.select(['arrdate', 'dtaddto']).withColumn('admission_time', F.datediff(F.col('dtaddto'), F.col('arrdate'))).filter(F.col('admission_time') < 89).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Logical exploration example 2: Filter airport data (TFA-8)\n",
    "The airport codes table has over 50k entries. We can reduce it by filtering for *type airport* and for matching with immigration data we also need the *iata_code*.\n",
    "\n",
    "The resulting dataframe from the airport dataset will be kept as a transformed version of the table (trimmed of spaces in column `iata_code` and exported to Parquet later (dimension table \"airport\").\n",
    "\n",
    "The code below creates five examples of travellers and their corresponding airport information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+----------+--------------------+------------+----------+-----------+--------------------+\n",
      "|airport_code|  cicid|   arrdate|                name|municipality|iso_region|iso_country|         coordinates|\n",
      "+------------+-------+----------+--------------------+------------+----------+-----------+--------------------+\n",
      "|         BGM| 154968|2016-04-01|Greater Binghamto...|  Binghamton|     US-NY|         US|-75.97979736, 42....|\n",
      "|         FMY|3147212|2016-04-17|          Page Field|  Fort Myers|     US-FL|         US|-81.8632965087999...|\n",
      "|         FMY|3366926|2016-04-18|          Page Field|  Fort Myers|     US-FL|         US|-81.8632965087999...|\n",
      "|         FMY|3148262|2016-04-17|          Page Field|  Fort Myers|     US-FL|         US|-81.8632965087999...|\n",
      "|         FMY|5200043|2016-04-27|          Page Field|  Fort Myers|     US-FL|         US|-81.8632965087999...|\n",
      "+------------+-------+----------+--------------------+------------+----------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all airport codes from the airport dataframe\n",
    "airport_table = ['iata_code', 'name', 'municipality', 'iso_region', 'iso_country', 'coordinates']\n",
    "airport_codes = air_df.filter(air_df.iata_code.isNotNull()).select(airport_table).withColumnRenamed('iata_code', 'airport_code')\n",
    "\n",
    "# Read Columns from immigration dataset\n",
    "imm_airports = imm_df.select(['cicid', 'arrdate', 'i94port']).withColumnRenamed('i94port', 'airport_code').distinct()\n",
    "\n",
    "# Clean the codes from any spaces\n",
    "airport_code = airport_codes.withColumn('airport_code', F.trim(F.col('airport_code')))\n",
    "imm_airports = imm_airports.withColumn('airport_code', F.trim(F.col('airport_code')))\n",
    "\n",
    "# Join with immigration dataset\n",
    "imm_airports.join(airport_codes, on='airport_code').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Logical exploration example 3: Renaming columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(imm_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
