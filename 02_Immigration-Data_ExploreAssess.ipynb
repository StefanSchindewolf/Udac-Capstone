{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Data ETL\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* **Step 2: Explore and Assess the Data**\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2 Explore and assess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Overview\n",
    "In this section we will explore the datasets chosen by **Flyby Salads** product management department using Apache Spark. We will idenfify and implement necessary transformations and finally export the data to S3 in parquet format.\n",
    "\n",
    "Subsections of this Notebook:\n",
    "* Preparations (imports, file locations, etc.)\n",
    "* Reading files using Spark\n",
    "* Data Exploration and description\n",
    "    * Dataframe schema analysis\n",
    "    * Data Quality analysis of the data\n",
    "    * Data Type transformation steps\n",
    "* Defining a new schema and storing it in JSON format\n",
    "* Logical Data exploration examples\n",
    "  (Analyzing table contents using PySpark to find more transformation actions)\n",
    "    * Creating an airport dimension table\n",
    "    * Linking airports to immigration data\n",
    "    * \n",
    "* Exporting the resulting tables to Parquet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note:**\n",
    "\n",
    "    * We want to keep track of transformation steps\n",
    "    * So if a required transformation is identified, it will be assigned a number like \"TFA-x\"\n",
    "    * Where \"TFA\" stands for transformation action (plus a number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from os.path import getsize\n",
    "from nb_helpers import summarize_data, get_sas_definitions, read_sas_in_chunks, \\\n",
    "                        read_csv_print, print_stat, non_iso_date_change, change_nullables\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s %(levelname)s \\t %(message)s ',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "log = logging.getLogger('log')\n",
    "\n",
    "# Improve view\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Reading data files into Spark Dataframes\n",
    "The next cell contains the file locations for each dataset, you may adjust it if required. Also note the configuration of the sample size which will improve performance while making some basic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Source file locations\n",
    "dem_file = 'us-cities-demographics.csv'\n",
    "imm_file = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "air_file = 'airport-codes_csv.csv'\n",
    "\n",
    "# Schema file locations\n",
    "air_schema_file = 'air_schema.json'\n",
    "dem_schema_file = 'dem_schema.json'\n",
    "imm_schema_file = 'imm_schema.json'\n",
    "\n",
    "\n",
    "# If working on a sample of the large immigration dataset, set your sample size here:\n",
    "sample_size = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "During my analysis I changed the schema of each dataset imported and based on the transformations done created a new schema definition. The definition is stored in a JSON file.\n",
    "\n",
    "The data will be **imported again** using those schema **at the end** of the \"Explore and Assess\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we setup the Spark Session and read all three files into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  3096313  lines from  ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "Reading  2891  lines from  us-cities-demographics.csv\n",
      "Reading  55075  lines from  airport-codes_csv.csv\n",
      "Operation runtime  0:00:27.576090\n"
     ]
    }
   ],
   "source": [
    "# Setup Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Read data from sources\n",
    "imm_df = spark.read \\\n",
    "                .format('com.github.saurfang.sas.spark') \\\n",
    "                .option(\"inferSchema\",\"true\") \\\n",
    "                .load(imm_file)\n",
    "imm_rows = imm_df.count()\n",
    "air_df = spark.read.csv(air_file, header=True)\n",
    "air_rows = air_df.count()\n",
    "dem_df = spark.read.csv(dem_file, header=True)\n",
    "dem_rows = dem_df.count()\n",
    "\n",
    "# List of our staging dataframes\n",
    "df_list = {'imm_df': imm_df, 'air_df': air_df, 'dem_df': dem_df}\n",
    "\n",
    "# Save a copy of initial total data, because we will work with a sample only in this notebook\n",
    "imm_full = imm_df\n",
    "\n",
    "# Output of results\n",
    "elapse = datetime.now() - start_time\n",
    "print('Reading ', imm_rows , ' lines from ', imm_file)\n",
    "print('Reading ', dem_rows, ' lines from ', dem_file)\n",
    "print('Reading ', air_rows, ' lines from ', air_file)\n",
    "print('Operation runtime ', elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Explore and Describe data\n",
    "This section contains the following subsections:\n",
    "* Spark schema analysis\n",
    "* Spark dataframe descriptions analysis\n",
    "\n",
    "Before we start this section we are naming each dataframe and reducing immigration data to a sample of the complete dataframe, keeping the full set in variable `imm_full` (the sample size being configured at the beginning of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_df = imm_full.sample(sample_size)\n",
    "imm_df.name = 'i94 immigration data'\n",
    "dem_df.name = 'city demographics'\n",
    "air_df.name = 'airport codes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Display schemas created automatically by Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imm_df.printSchema()\n",
    "air_df.printSchema()\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note** (TFA-1) Several columns in the i94 dataframe were infered as double while they should be of data type integer. Those types should be changed (see `cicid, i94yr,...`).\n",
    "\n",
    "**Note** (TFA-2) Several columns in airport and demographic data were infered as string where they should be either double or integer (see airport data, `elevation_ft` or demographic data `median_age`\n",
    "\n",
    "**Note** (TFA-3) Demographics data file was imported as a one-column file due to ';' being used as delimiter. We read the file again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dem_df = spark.read.csv(dem_file, header=True, sep=';')\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note**: (TFA-4) For easier handling we strip columns of spaces, apply lower case and replace inner spaces and '-' with underscore '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def trim_col_headers(df):\n",
    "    for col in df.columns:\n",
    "        changed_col = col.strip().lower().replace(' ', '_').replace('-', '_')\n",
    "        df = df.withColumnRenamed(col, changed_col)\n",
    "    return df\n",
    "\n",
    "dem_df = trim_col_headers(dem_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dataset descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Statistical summary on data\n",
    "The descriptive statistics were shown in Step 1 already, so we will skip this step here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Missing values per dataset\n",
    "First we are measuring the amount of empty cells per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-043bd3ee": {
       "style": "primary"
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get total amount of entries per column\n",
    "imm_col_count = imm_full.describe().filter(F.col('summary') == 'count')\n",
    "air_col_count = air_df.describe().filter(F.col('summary') == 'count')\n",
    "dem_col_count = dem_df.describe().filter(F.col('summary') == 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|summary|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|\n",
      "+-------+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|  count|100.0|100.0| 100.0| 100.0| 100.0|  100.0|  100.0|  100.0|   95.1|   95.4| 100.0|  100.0|100.0|   100.0|    39.2|  0.3|  100.0|   95.5|    0.0|   95.5|  100.0|  100.0|  86.6|   3.7|   97.3| 100.0| 99.4|   100.0|\n",
      "+-------+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "\n",
      "None\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|summary|ident| type| name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|  count|100.0|100.0|100.0|        87.3|    100.0|      100.0|     100.0|        89.7|    74.5|     16.7|      52.1|      100.0|\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n",
      "None\n",
      "+-------+-----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----+-----+\n",
      "|summary| city|state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code| race|count|\n",
      "+-------+-----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----+-----+\n",
      "|  count|100.0|100.0|     100.0|           99.9|             99.9|           100.0|              99.6|        99.6|                  99.4|     100.0|100.0|100.0|\n",
      "+-------+-----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Divide number of entries per column by total dataframe rows to get a percentage value\n",
    "for df, numrows in zip([imm_col_count, air_col_count, dem_col_count], \\\n",
    "                       [imm_rows, air_rows, dem_rows]):\n",
    "    for column in df.columns:\n",
    "        if column == 'summary':\n",
    "            next\n",
    "        else:\n",
    "            df = df.withColumn(column, F.col(column).cast('int'))\n",
    "            df = df.withColumn(column, F.bround((F.col(column) / numrows * 100), scale=1))\n",
    "    print('{}'.format(df.show()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Unique values per column\n",
    "We can count the number of unique values per dataframe or column using `dropDuplicates()` and `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique entries in table \"cicid\": 31085\n",
      "No. of unique entries in table \"i94yr\": 1\n",
      "No. of unique entries in table \"i94mon\": 1\n",
      "No. of unique entries in table \"i94cit\": 182\n",
      "No. of unique entries in table \"i94res\": 186\n",
      "No. of unique entries in table \"i94port\": 148\n",
      "No. of unique entries in table \"arrdate\": 30\n",
      "No. of unique entries in table \"i94mode\": 5\n",
      "No. of unique entries in table \"i94addr\": 93\n",
      "No. of unique entries in table \"depdate\": 173\n",
      "No. of unique entries in table \"i94bir\": 96\n",
      "No. of unique entries in table \"i94visa\": 3\n",
      "No. of unique entries in table \"count\": 1\n",
      "No. of unique entries in table \"dtadfile\": 65\n",
      "No. of unique entries in table \"visapost\": 247\n",
      "No. of unique entries in table \"occup\": 21\n",
      "No. of unique entries in table \"entdepa\": 13\n",
      "No. of unique entries in table \"entdepd\": 11\n",
      "No. of unique entries in table \"entdepu\": 2\n",
      "No. of unique entries in table \"matflag\": 2\n",
      "No. of unique entries in table \"biryear\": 96\n",
      "No. of unique entries in table \"dtaddto\": 274\n",
      "No. of unique entries in table \"gender\": 5\n",
      "No. of unique entries in table \"insnum\": 234\n",
      "No. of unique entries in table \"airline\": 178\n",
      "No. of unique entries in table \"admnum\": 31083\n",
      "No. of unique entries in table \"fltno\": 1929\n",
      "No. of unique entries in table \"visatype\": 15\n",
      "No. of unique entries in table \"ident\": 55075\n",
      "No. of unique entries in table \"type\": 7\n",
      "No. of unique entries in table \"name\": 52144\n",
      "No. of unique entries in table \"elevation_ft\": 5450\n",
      "No. of unique entries in table \"continent\": 7\n",
      "No. of unique entries in table \"iso_country\": 244\n",
      "No. of unique entries in table \"iso_region\": 2810\n",
      "No. of unique entries in table \"municipality\": 27134\n",
      "No. of unique entries in table \"gps_code\": 40851\n",
      "No. of unique entries in table \"iata_code\": 9043\n",
      "No. of unique entries in table \"local_code\": 27437\n",
      "No. of unique entries in table \"coordinates\": 54874\n",
      "No. of unique entries in table \"city\": 567\n",
      "No. of unique entries in table \"state\": 49\n",
      "No. of unique entries in table \"median_age\": 180\n",
      "No. of unique entries in table \"male_population\": 594\n",
      "No. of unique entries in table \"female_population\": 595\n",
      "No. of unique entries in table \"total_population\": 594\n",
      "No. of unique entries in table \"number_of_veterans\": 578\n",
      "No. of unique entries in table \"foreign_born\": 588\n",
      "No. of unique entries in table \"average_household_size\": 162\n",
      "No. of unique entries in table \"state_code\": 49\n",
      "No. of unique entries in table \"race\": 5\n",
      "No. of unique entries in table \"count\": 2785\n"
     ]
    }
   ],
   "source": [
    "# Count **distinct** entries per column and print\n",
    "for df, numrows in zip([imm_df, air_df, dem_df], [imm_rows, air_rows, dem_rows]):\n",
    "    for column in df.columns:\n",
    "        distinct_count = df.select(F.col(column)).distinct().count()\n",
    "        print('No. of unique entries in table \"{}\": {}'.format(column, distinct_count))\n",
    "    # print('{}'.format(df.show()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Transformations\n",
    "This section will contain the steps required to transform the datasets.\n",
    "\n",
    "From previous notes we now carry out some transformation actions.\n",
    "1. Double to Integer (TFA-1)\n",
    "1. String to numbers (TFA-2)\n",
    "1. SAS Date Type to date (TFA-5)\n",
    "1. Non-Iso Date Type to date (TFA-6)\n",
    "\n",
    "Note that TFA-3 and TFA-4 were already carried out in the previous chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Integer Type Transformation (TFA-1)\n",
    "The fields mentioned here are IntegerType but were infered as double, we are changing them to integer.\n",
    "\n",
    "In addition the string column `entdepa` will also be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, switched column cicid from DoubleType to IntegerType\n",
      "Done, switched column i94yr from DoubleType to IntegerType\n",
      "Done, switched column i94mon from DoubleType to IntegerType\n",
      "Done, switched column i94cit from DoubleType to IntegerType\n",
      "Done, switched column i94res from DoubleType to IntegerType\n",
      "Done, switched column arrdate from DoubleType to IntegerType\n",
      "Done, switched column depdate from DoubleType to IntegerType\n",
      "Done, switched column i94mode from DoubleType to IntegerType\n",
      "Done, switched column i94bir from DoubleType to IntegerType\n",
      "Done, switched column i94visa from DoubleType to IntegerType\n",
      "Done, switched column count from DoubleType to IntegerType\n",
      "Done, switched column biryear from DoubleType to IntegerType\n",
      "+-----+-----+------+------+------+-------+-------+-------+------+-------+-----+-------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|arrdate|depdate|i94mode|i94bir|i94visa|count|biryear|\n",
      "+-----+-----+------+------+------+-------+-------+-------+------+-------+-----+-------+\n",
      "|    6| 2016|     4|   692|   692|  20573|   null|   null|    37|      2|    1|   1979|\n",
      "|    7| 2016|     4|   254|   276|  20551|   null|      1|    25|      3|    1|   1991|\n",
      "|   15| 2016|     4|   101|   101|  20545|  20691|      1|    55|      2|    1|   1961|\n",
      "|   16| 2016|     4|   101|   101|  20545|  20567|      1|    28|      2|    1|   1988|\n",
      "|   17| 2016|     4|   101|   101|  20545|  20567|      1|     4|      2|    1|   2012|\n",
      "+-----+-----+------+------+------+-------+-------+-------+------+-------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Done, switched column admnum from DoubleType to LongType\n",
      "+-----------+\n",
      "|     admnum|\n",
      "+-----------+\n",
      "| 1897628485|\n",
      "| 3736796330|\n",
      "|  666643185|\n",
      "|92468461330|\n",
      "|92468463130|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "int_columns = ['cicid','i94yr', 'i94mon', 'i94cit', 'i94res', \\\n",
    "              'arrdate', 'depdate', 'i94mode', 'i94bir', \\\n",
    "              'i94visa', 'count', 'biryear']\n",
    "\n",
    "def change_type(df=None, fields=None, totype=None):\n",
    "    if ((totype is not None) and (fields is not None)):\n",
    "        for column in fields:\n",
    "            cur_type = df.schema[column]\n",
    "            df = df.withColumn(column, F.col(column).cast(totype))\n",
    "            new_type = df.schema[column]\n",
    "            print('Done, switched column {} from {} to {}' \\\n",
    "                  .format(column, cur_type.dataType, new_type.dataType))\n",
    "        print(df.select(fields).show(5))\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "imm_df = change_type(imm_df, int_columns, 'int')\n",
    "imm_df = change_type(imm_df, ['admnum'], 'bigint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### String to numbers (TFA-2)\n",
    "In dataframes for demographic and airport data no float types were infered. Instead we see floating point numbers imported as strings.\n",
    "Those will be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, switched column elevation_ft from StringType to DoubleType\n",
      "+------------+\n",
      "|elevation_ft|\n",
      "+------------+\n",
      "|        11.0|\n",
      "|      3435.0|\n",
      "|       450.0|\n",
      "|       820.0|\n",
      "|       237.0|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Airport data:\n",
    "float_columns = ['elevation_ft']\n",
    "int_columns = None\n",
    "\n",
    "air_df = change_type(air_df, float_columns, 'double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, switched column median_age from StringType to DoubleType\n",
      "Done, switched column average_household_size from StringType to DoubleType\n",
      "Done, switched column count from StringType to DoubleType\n",
      "+----------+----------------------+-------+\n",
      "|median_age|average_household_size|  count|\n",
      "+----------+----------------------+-------+\n",
      "|      33.8|                   2.6|25924.0|\n",
      "|      41.0|                  2.39|58723.0|\n",
      "|      38.5|                  2.58| 4759.0|\n",
      "|      34.5|                  3.18|24437.0|\n",
      "|      34.6|                  2.73|76402.0|\n",
      "+----------+----------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Done, switched column male_population from StringType to IntegerType\n",
      "Done, switched column female_population from StringType to IntegerType\n",
      "Done, switched column total_population from StringType to IntegerType\n",
      "Done, switched column number_of_veterans from StringType to IntegerType\n",
      "Done, switched column foreign_born from StringType to IntegerType\n",
      "+---------------+-----------------+----------------+------------------+------------+\n",
      "|male_population|female_population|total_population|number_of_veterans|foreign_born|\n",
      "+---------------+-----------------+----------------+------------------+------------+\n",
      "|          40601|            41862|           82463|              1562|       30908|\n",
      "|          44129|            49500|           93629|              4147|       32935|\n",
      "|          38040|            46799|           84839|              4819|        8229|\n",
      "|          88127|            87105|          175232|              5821|       33878|\n",
      "|         138040|           143873|          281913|              5829|       86253|\n",
      "+---------------+-----------------+----------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Demographic Data\n",
    "float_columns = ['median_age', 'average_household_size', 'count' ]\n",
    "int_columns = ['male_population', 'female_population', \\\n",
    "               'total_population', 'number_of_veterans', 'foreign_born']\n",
    "\n",
    "dem_df = change_type(dem_df, float_columns, 'double')\n",
    "dem_df = change_type(dem_df, int_columns, 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### SAS Date Type Transformations (TFA-5)\n",
    "The SAS formatted dates (e.v. \"20573.0\") should be changed to proper date types.\n",
    "Note we are not putting this into the nb_helpers.py because it' s a more exemplary kind of transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|   arrdate|   depdate|\n",
      "+----------+----------+\n",
      "|2016-04-29|      null|\n",
      "|2016-04-07|      null|\n",
      "|2016-04-01|2016-08-25|\n",
      "|2016-04-01|2016-04-23|\n",
      "|2016-04-01|2016-04-23|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sas_date_columns = ['arrdate', 'depdate']\n",
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "def sasdate_to_date(df=None, column_list=None):\n",
    "    # SAS has its own epoch, which we add temporarily as a column\n",
    "    df = df.withColumn('epoch_start', F.lit(\"01-01-1960 00:00:00\"))\n",
    "    df = df.withColumn('epoch_start', F.to_date(F.col('epoch_start'), \"dd-M-yyyy\"))\n",
    "    # Check each column and convert double to int, then add this int to epoch_start\n",
    "    for column in column_list:\n",
    "        df = df.withColumn(column, F.col(column).cast('int'))\n",
    "        stm = 'date_add(epoch_start, {})'.format(column)\n",
    "        df = df.withColumn(column, F.expr(stm))\n",
    "    return df\n",
    "\n",
    "imm_df = sasdate_to_date(imm_df, sas_date_columns)\n",
    "imm_df.select(sas_date_columns).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Non-Iso Date Type Transformations (TFA-6)\n",
    "Columns `dtadtto` and `dtadtofile` contain non-iso dateformats and will be converted into proper date types as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Changing SAS Date Type to Spark Datetype\n",
      "Done, switched column dtaddto from StringType to DateType\n",
      "Done, switched column dtadfile from StringType to DateType\n",
      "Here a view on some examples: \n",
      "+----------+\n",
      "|   dtaddto|\n",
      "+----------+\n",
      "|2016-10-28|\n",
      "|      null|\n",
      "|2016-09-30|\n",
      "|2016-09-30|\n",
      "|2016-09-30|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|  dtadfile|\n",
      "+----------+\n",
      "|      null|\n",
      "|2013-08-11|\n",
      "|2016-04-01|\n",
      "|2016-04-01|\n",
      "|2016-04-01|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "# if required, update the dictionary with a new column and a new pattern or change existing\n",
    "imm_non_iso_dates = {'dtaddto': 'MMddyyyy', 'dtadfile': 'yyyyMMdd'}    \n",
    "imm_df = non_iso_date_change(imm_df, imm_non_iso_dates)\n",
    "\n",
    "print('Here a view on some examples: ')\n",
    "print(imm_df.select('dtaddto').show(5), imm_df.select('dtadfile').show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Missing values and nullable setting (TFA-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Changing nullable setting\n",
    "Changing columns in the i94 dataset to `nullable = true` where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, Schema changed for  2  columns.\n"
     ]
    }
   ],
   "source": [
    "# Define fields which should not be empty\n",
    "not_null_fields = ['cicid', 'admnum']\n",
    "imm_df = imm_df.dropna(subset=not_null_fields)\n",
    "imm_df = change_nullables(session=spark, df=imm_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Demographic file did not have any missing values, so we set all fields to nullable = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, Schema changed for  12  columns.\n"
     ]
    }
   ],
   "source": [
    "not_null_fields = dem_df.columns\n",
    "dem_df = dem_df.na.drop(subset=not_null_fields)\n",
    "dem_df = change_nullables(session=spark, df=dem_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From airport data we want the items with iata code filled only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, Schema changed for  1  columns.\n"
     ]
    }
   ],
   "source": [
    "not_null_fields = ['iata_code']\n",
    "air_df = air_df.na.drop(subset=not_null_fields)\n",
    "air_df = change_nullables(session=spark, df=air_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Exporting a PySpark schema to JSON\n",
    "**(optional: re-importing data with updated schema from JSON)**\n",
    "Now that we have altered the dataframe schemas we export those schemas into JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write to Json file using a function\n",
    "def write_schema_json(df, schema_file):\n",
    "    import json\n",
    "    import sys\n",
    "    with open(schema_file, \"w\") as f:\n",
    "        json.dump(df.schema.jsonValue(), f)\n",
    "\n",
    "# Let's call that for each dataframe\n",
    "write_schema_json(imm_df, imm_schema_file)\n",
    "write_schema_json(dem_df, dem_schema_file)\n",
    "write_schema_json(air_df, air_schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**At this point we could read the JSON schema and apply it for importing data.**\n",
    "\n",
    "*However during my work I got a ClassCastException after importing the data on \".show()\" or any other actions.\n",
    "This is why the following cell is turned into markdown and exempted from execution*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#Save some lines by using a function to import a schema from JSON\n",
    "\n",
    "`def import_schema(schema_file):`   \n",
    "    `import json\n",
    "    schema_df = spark.sparkContext.wholeTextFiles(schema_file)\n",
    "    schema_content = schema_df.collect()[0][1]\n",
    "    schema_dict = json.loads(str(schema_content))\n",
    "    new_schema = T.StructType.fromJson(schema_dict)\n",
    "    return new_schema`\n",
    "\n",
    "#Now apply the schema for each\n",
    "`start_time = datetime.now()\n",
    "imm_df = spark.read.format('com.github.saurfang.sas.spark').load(path=imm_file, schema=import_schema(imm_schema_file))\n",
    "air_df = spark.read.csv(air_file, header=True, schema=import_schema(air_schema_file))\n",
    "dem_df = spark.read.csv(dem_file, header=True, sep=';', schema=import_schema(dem_schema_file))\n",
    "elapse = datetime.now() - start_time\n",
    "print('Reading ', imm_df.count(), ' lines from ', imm_file)\n",
    "print('Reading ', dem_df.count(), ' lines from ', dem_file)\n",
    "print('Reading ', air_df.count(), ' lines from ', air_file)\n",
    "print('Operation runtime ', elapse)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Resulting schema definitions\n",
    "In the sections above we have transformed specific values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = false)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: date (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: date (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- dtadfile: date (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: date (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: long (nullable = false)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- epoch_start: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: double (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = false)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = false)\n",
      " |-- state: string (nullable = false)\n",
      " |-- median_age: double (nullable = false)\n",
      " |-- male_population: integer (nullable = false)\n",
      " |-- female_population: integer (nullable = false)\n",
      " |-- total_population: integer (nullable = false)\n",
      " |-- number_of_veterans: integer (nullable = false)\n",
      " |-- foreign_born: integer (nullable = false)\n",
      " |-- average_household_size: double (nullable = false)\n",
      " |-- state_code: string (nullable = false)\n",
      " |-- race: string (nullable = false)\n",
      " |-- count: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imm_df.printSchema()\n",
    "air_df.printSchema()\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Logical exploration using PySpark\n",
    "In this chapter we will show some more explorative analysis that we help us understanding the data.\n",
    "\n",
    "**While doing this we will carry out more transformations. Those will be designated again as \"TFA-x\" to keep track of them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 01 - Cleaning airport data to get an airport dimension table (TFA-8)\n",
    "The airport codes table has over 50k entries. We can reduce it by filtering for *type airport* and for matching with immigration data we also need the *iata_code*.\n",
    "\n",
    "The resulting dataframe from the airport dataset will be kept as a transformed version of the table (trimmed of spaces in column `iata_code` and exported to Parquet later (dimension table \"airport\").\n",
    "\n",
    "The code below creates five examples of travellers and their corresponding airport information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully joined lines: 20436 \t\t\tJoin failed on 10727 lines\n",
      "+-------+----------+-------+---------+--------------------+------------+----------+-----------+------------------+-----------+\n",
      "|  cicid|   arrdate|airport|iata_code|                name|municipality|iso_region|iso_country|          latitude|  longitude|\n",
      "+-------+----------+-------+---------+--------------------+------------+----------+-----------+------------------+-----------+\n",
      "|4041803|2016-04-22|    BGM|      BGM|Greater Binghamto...|  Binghamton|     US-NY|         US|      -75.97979736|42.20869827|\n",
      "|  93624|2016-04-01|    FMY|      FMY|          Page Field|  Fort Myers|     US-FL|         US|-81.86329650879999|26.58659935|\n",
      "| 818333|2016-04-05|    FMY|      FMY|          Page Field|  Fort Myers|     US-FL|         US|-81.86329650879999|26.58659935|\n",
      "|2553357|2016-04-14|    FMY|      FMY|          Page Field|  Fort Myers|     US-FL|         US|-81.86329650879999|26.58659935|\n",
      "|2740149|2016-04-15|    FMY|      FMY|          Page Field|  Fort Myers|     US-FL|         US|-81.86329650879999|26.58659935|\n",
      "+-------+----------+-------+---------+--------------------+------------+----------+-----------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read all available airport codes from the airport dataframe\n",
    "airports_fields = ['iata_code', 'name', 'municipality', 'iso_region', \\\n",
    "                   'iso_country', 'coordinates']\n",
    "dim_airports = air_df.filter(air_df.iata_code.isNotNull()).select(airports_fields)\n",
    "\n",
    "# Split the coordinates column into latitude and longitude, remove spaces\n",
    "dim_airports = dim_airports.withColumn('latitude', F.split(dim_airports['coordinates'], ',').getItem(0)).withColumn('longitude', F.split(dim_airports['coordinates'], ',').getItem(1))\n",
    "dim_airports = dim_airports.withColumn('iata_code', F.trim(F.col('iata_code'))).withColumn('latitude', F.trim(F.col('latitude'))).withColumn('longitude', F.trim(F.col('longitude')))\n",
    "dim_airports = dim_airports.drop('coordinates')\n",
    "dim_airports.name = 'dim_airports'\n",
    "\n",
    "# TEST if data can be matched\n",
    "# Read some columns from immigration dataset including the airport codes, remove spaces\n",
    "imm_airports = imm_df.select(['cicid', 'arrdate', 'i94port']).withColumnRenamed('i94port', 'airport').distinct()\n",
    "imm_airports = imm_airports.withColumn('airport', F.trim(F.col('airport')))\n",
    "\n",
    "# Join with immigration dataset\n",
    "leftouter_set = imm_airports.join(dim_airports, dim_airports.iata_code == imm_airports.airport, 'leftouter')\n",
    "matching = leftouter_set.select(['iata_code']).filter(F.col('iata_code').isNotNull()).count()\n",
    "not_matching = leftouter_set.select(['iata_code']).filter(F.col('iata_code').isNull()).count()\n",
    "print('Successfully joined lines: {} \\t\\t\\tJoin failed on {} lines'.format(matching, (not_matching)))\n",
    "print(leftouter_set.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This catches **only about 65% of the entries** in our fact table.\n",
    "\n",
    "Now we could also try to match the city name in the SAS values list to the airport`municipality` column, but we consider this even worse.\n",
    "\n",
    "Therefore we check the airport codes in the immigration table which are not matching to column `i94port` and **count missed joins per airport**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|airport|count|\n",
      "+-------+-----+\n",
      "|    NYC| 4917|\n",
      "|    HHW| 1374|\n",
      "|    CHI| 1286|\n",
      "|    FTL|  985|\n",
      "|    LVG|  906|\n",
      "|    WAS|  771|\n",
      "|    SAI|  224|\n",
      "|    SAJ|   84|\n",
      "|    NOL|   50|\n",
      "|    YGF|   30|\n",
      "|    XXX|   29|\n",
      "|    X96|   26|\n",
      "|    PHU|   20|\n",
      "|    YHC|    7|\n",
      "|    INP|    5|\n",
      "|    PHR|    3|\n",
      "|    MIL|    3|\n",
      "|    JKM|    3|\n",
      "|    X44|    1|\n",
      "|    EPI|    1|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get list of missing entries where airport code could not match\n",
    "not_matching = leftouter_set.select(['airport', 'iata_code']).filter(F.col('iata_code').isNull())\n",
    "# Count number of missing entries\n",
    "not_matching.groupBy('airport').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**This list sets a priority for cleaning up our airport dataframe or immigration data.**\n",
    "\n",
    "*Example: if we find out why \"NYC\" is not matching an entry of our airport CSV list we would have an additional 4.888 joins with our fact table(!)*\n",
    "\n",
    "One way to improve data quality on airports would be updating the airport code table with those missing codes and their corresponding city names. Another way would be to check if the airport code in the immigration data is correct at all.\n",
    "\n",
    "*At least for our example \"NYC\" it seems that this is not a proper iata_code. New York's airports have individual codes like \"LGA\" for LaGuardia Airport.*\n",
    "\n",
    "*This indicates that we should **correct the data in the immigration table** with the correct value. This requires however to **somehow derive the correct iata_code from other columns in the table**.\n",
    "\n",
    "I will take this data quality action **out of scope**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 02 - Create status code dimension table (TFA-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+--------------+\n",
      "|entdepa|entdepd|entdepu|matflag|status_flag_id|\n",
      "+-------+-------+-------+-------+--------------+\n",
      "|      H|      O|      U|      M|  120259084288|\n",
      "|      T|      O|      U|      M|  146028888064|\n",
      "|      Z|      O|      U|      M|  214748364800|\n",
      "|      T|      N|      U|      M|  523986010112|\n",
      "|      G|      I|      U|      M|  644245094400|\n",
      "|      G|      Q|      U|      M|  712964571136|\n",
      "|      Z|      I|      U|      M|  876173328384|\n",
      "|      G|      N|      U|      M|  919123001344|\n",
      "|      O|      N|      U|      M| 1211180777472|\n",
      "|      O|      I|      U|      M| 1228360646656|\n",
      "|      Z|      K|      U|      M| 1254130450432|\n",
      "|      G|      J|      U|      M| 1520418422784|\n",
      "|      T|      K|      U|      M| 1623497637888|\n",
      "|      Z|      R|      U|      M| 1632087572480|\n",
      "|      G|      O|      U|      M| 1640677507072|\n",
      "|      O|      O|      U|      M| 1675037245440|\n",
      "+-------+-------+-------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract unique entries of status flags from immigration dataframe\n",
    "status_columns = ['entdepa', 'entdepd', 'entdepu', 'matflag']\n",
    "\n",
    "dim_status = imm_full.select(status_columns) \\\n",
    "                .dropna() \\\n",
    "                .dropDuplicates() \\\n",
    "                .withColumn(\"status_flag_id\", \\\n",
    "                            F.monotonically_increasing_id()) \n",
    "dim_status.name = 'dim_status'\n",
    "dim_status.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we check if we can match those to our airports dataframe using the short code in column `i94port`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 03 - Time Data dimension table (TFA-10)\n",
    "We create a dimension table from the provided immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table dim_time has 837 entries:\n",
      "+----------+------------+-----------+----+-----+----+\n",
      "| datestamp|day_of_month|day_of_year|week|month|year|\n",
      "+----------+------------+-----------+----+-----+----+\n",
      "|2016-08-16|          16|        229|  33|    8|2016|\n",
      "|2016-09-16|          16|        260|  37|    9|2016|\n",
      "|2018-04-17|          17|        107|  16|    4|2018|\n",
      "|2017-02-27|          27|         58|   9|    2|2017|\n",
      "|2016-06-04|           4|        156|  22|    6|2016|\n",
      "+----------+------------+-----------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the relevant fields where to collect date entries for time dimension table\n",
    "dim_time_columns = ['arrdate', 'depdate', 'dtaddto', 'dtadfile']\n",
    "\n",
    "def create_dim_time(df=None, columns=None):\n",
    "    \"\"\" This functions stacks given columns containing dates\n",
    "        upon another.\n",
    "        Then it derives from those dates the days, weeks, months and the year.\n",
    "        \n",
    "        Returns: Dataframe with time dimension table\n",
    "        \"\"\"\n",
    "    \n",
    "    # Reduce given df to the provided columns and remove duplicates\n",
    "    dates_in_col = df.select(columns).dropDuplicates()\n",
    "\n",
    "    # Create a dataframe containing content of the 1st data column\n",
    "    dim_time = spark.createDataFrame(dates_in_col.select(dim_time_columns[0]).distinct().collect())\n",
    "\n",
    "    # Iterate through the other columns and unionize them with the first one (by one :-)\n",
    "    for next_col in dim_time_columns[1:]:\n",
    "        temp_df = spark.createDataFrame(dates_in_col.select(next_col).collect())\n",
    "        dim_time = dim_time.union(temp_df).distinct()\n",
    "            \n",
    "    # Calculate remaining colums from data in 1st column\n",
    "    dim_time = dim_time.withColumnRenamed(dim_time_columns[0], 'datestamp')\n",
    "    dim_time = dim_time \\\n",
    "                .withColumn('day_of_month', F.dayofmonth('datestamp')) \\\n",
    "                .withColumn('day_of_year', F.dayofyear('datestamp')) \\\n",
    "                .withColumn('week', F.weekofyear('datestamp')) \\\n",
    "                .withColumn('month', F.month('datestamp')) \\\n",
    "                .withColumn('year', F.year('datestamp')) \\\n",
    "                .dropDuplicates()\n",
    "    return dim_time\n",
    "\n",
    "#dim_time = create_dim_time(imm_df, dim_time_columns)\n",
    "dim_time = create_dim_time(imm_df, dim_time_columns)\n",
    "dim_time.name = 'dim_time'\n",
    "\n",
    "\n",
    "print('Table {} has {} entries:'.format(dim_time.name, dim_time.count()))\n",
    "print(dim_time.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 04 - Demographics dimension table (TFA-11)\n",
    "\n",
    "We aggregate the demographics data per state (column `state_code`) and match it to the immigration data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------------+----------------+------------------+------------+----------------------+\n",
      "|state_code|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|\n",
      "+----------+---------------+-----------------+----------------+------------------+------------+----------------------+\n",
      "|        AZ|       11137275|         11360435|        22497710|           1322525|     3411565|     2.771264313123426|\n",
      "|        SC|        1265291|          1321685|         2586976|            163334|      134019|     2.471309525098029|\n",
      "|        LA|        3134990|          3367985|         6502975|            348855|      417095|    2.4628252838124087|\n",
      "+----------+---------------+-----------------+----------------+------------------+------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select ALL columns for this dimension table into dimension table\n",
    "demographics_columns = dem_df.columns\n",
    "dim_demographics = dem_df.select(demographics_columns).dropDuplicates()\n",
    "\n",
    "# Define grouping function per column \n",
    "cols_to_mean = 'average_household_size'\n",
    "cols_to_sum = ['state_code', 'male_population', 'female_population', \\\n",
    "               'total_population', 'number_of_veterans', 'foreign_born']\n",
    "\n",
    "# Calculate weighted average per state (arithmetic average not applicable)\n",
    "def weighted_average(df=None, mean_column=None, weight_column=None, agg_level=None):\n",
    "    \"\"\" Calculate weighted average for a dataframe.\n",
    "        You need to provide the column which stores the average values\n",
    "        and the column storing the weights.\n",
    "        You can specify an aggregation level which is a column by which the\n",
    "        data will be grouped (resulting in weighted averages for each group)\n",
    "        \n",
    "        Returns: Dataframe with weighted average column\n",
    "        \"\"\"\n",
    "    # Reduce to required columns:\n",
    "    df = df.select([mean_column, weight_column, agg_level]) \\\n",
    "            .withColumnRenamed(mean_column, 'mean_col') \\\n",
    "            .withColumnRenamed(weight_column, 'weight_col') \\\n",
    "            .withColumnRenamed(agg_level, 'agg_col')\n",
    "    \n",
    "    # Sum up weight column grouped by aggregation level\n",
    "    sum_of_weights = df.select(['agg_col', 'weight_col']).groupBy('agg_col').sum()\n",
    "    sum_of_weights = sum_of_weights.withColumnRenamed('sum(weight_col)', 'sum_weights')\n",
    "    \n",
    "    # Calculate product of individual means multiplied by weights\n",
    "    mean_times_weight = df.withColumn('mean_weight_product', (F.col('mean_col') * (F.col('weight_col'))))\n",
    "    # Sum up all mean values per agg level\n",
    "    sum_of_means = mean_times_weight.select(['agg_col', 'mean_weight_product']).groupBy('agg_col').sum()\n",
    "    sum_of_means = sum_of_means.withColumnRenamed('sum(mean_weight_product)', 'sum_of_meanpr')\n",
    "    \n",
    "    # Divide aggregated means by total weights per agg level\n",
    "    weighted_mean = sum_of_means \\\n",
    "                    .join(sum_of_weights, 'agg_col') \\\n",
    "                    .withColumn('weighted_avg', (F.col('sum_of_meanpr') / F.col('sum_weights')))\n",
    "    # Send result back\n",
    "    return_df = spark.createDataFrame(weighted_mean.select(['agg_col', 'weighted_avg']).collect())\n",
    "    return return_df\n",
    "\n",
    "\n",
    "weighted_mean_df = weighted_average(dim_demographics, cols_to_mean, 'total_population', 'state_code')\n",
    "summed_columns_df = dim_demographics.select(cols_to_sum).groupBy('state_code').sum()\n",
    "\n",
    "dim_demographics = summed_columns_df.join(weighted_mean_df, weighted_mean_df.agg_col == summed_columns_df.state_code)\n",
    "dim_demographics = dim_demographics.drop('agg_col') \\\n",
    "                    .withColumnRenamed('sum(male_population)', 'male_population') \\\n",
    "                    .withColumnRenamed('sum(female_population)', 'female_population') \\\n",
    "                    .withColumnRenamed('sum(total_population)', 'total_population') \\\n",
    "                    .withColumnRenamed('sum(number_of_veterans)', 'number_of_veterans') \\\n",
    "                    .withColumnRenamed('sum(foreign_born)', 'foreign_born') \\\n",
    "                    .withColumnRenamed('weighted_avg', 'average_household_size')\n",
    "\n",
    "dim_demographics.name = 'dim_demographics'\n",
    "dim_demographics.show(3)                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 05 - Immigration Fact Table (TFA-12\n",
    "\n",
    "We now prepare the fact table for export. During this the fact table is updated with a status code for dimension table `dim_status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+--------------+-------+-----+----------+------------+-------+-----+--------+---+------+-----------+-----------+-----+\n",
      "|  cicid|status| adm_number|transport_mode|airport|state|arrival_dt|departure_dt|airline|fltno|visatype|age|gender|res_country|cit_country|occup|\n",
      "+-------+------+-----------+--------------+-------+-----+----------+------------+-------+-----+--------+---+------+-----------+-----------+-----+\n",
      "|2645795|  null|93511289830|             1|    LOS|   CA|2016-04-14|        null|     Y4|00912|       2| 35|     M|        582|        582| null|\n",
      "|5682339|  null|59541506933|             1|    SPM|   MN|2016-04-30|        null|     DL|00163|       1| 60|     M|        131|        123| null|\n",
      "|     52|  null|92469910630|             1|    NYC|   NY|2016-04-01|  2016-04-14|     AB|07450|       2| 54|     M|        112|        101| null|\n",
      "|     79|  null|55433479933|             1|    BOS|   MA|2016-04-01|  2016-04-05|     LH|00422|       1| 51|     M|        103|        103| null|\n",
      "|    131|  null|55421638333|             1|    NEW|   NY|2016-04-01|  2016-04-03|     OS|00089|       2| 50|     M|        103|        103| null|\n",
      "+-------+------+-----------+--------------+-------+-----+----------+------------+-------+-----+--------+---+------+-----------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns which are used to join with dim_status\n",
    "join_status_condition = [dim_status.entdepa == imm_df.entdepa, dim_status.entdepu == imm_df.entdepu, \\\n",
    "                         dim_status.entdepd == imm_df.entdepd, dim_status.matflag == imm_df.matflag]\n",
    "join_columns = ['entdepa', 'entdepu', 'entdepd', 'matflag']\n",
    "\n",
    "fact_df = imm_df.join(dim_status, join_columns, 'leftouter')\n",
    "\n",
    "immigration_facts = fact_df.withColumn(\"cicid\", F.col(\"cicid\").cast(\"integer\")) \\\n",
    "                    .withColumn('status', F.col('status_flag_id')) \\\n",
    "                    .withColumnRenamed('admnum', 'adm_number') \\\n",
    "                    .withColumnRenamed('arrdate','arrival_dt') \\\n",
    "                    .withColumnRenamed('depdate','departure_dt') \\\n",
    "                    .withColumn(\"cit_country\", F.col(\"i94cit\").cast(\"integer\")) \\\n",
    "                    .withColumn(\"res_country\", F.col(\"i94res\").cast(\"integer\")) \\\n",
    "                    .withColumnRenamed(\"i94port\", \"airport\") \\\n",
    "                    .withColumn(\"transport_mode\", F.col(\"i94mode\").cast(\"integer\")) \\\n",
    "                    .withColumnRenamed(\"i94addr\", \"state\") \\\n",
    "                    .withColumn(\"age\", F.col(\"i94bir\").cast(\"integer\")) \\\n",
    "                    .withColumn(\"visatype\", F.col(\"i94visa\").cast(\"integer\")) \\\n",
    "                    .drop('i94yr', 'i94mon', 'i94cit', 'i94res', 'arrdate', \\\n",
    "                          'depdate', 'count', 'admnum', 'i94mode','i94bir' \\\n",
    "                          'i94bir', 'dtadfile', 'epoch_start', 'status'\n",
    "                          'dtadfile', 'visapost', 'dtaddto', 'insnum', 'i94visa')\n",
    "\n",
    "imm_df_ommitted_columns = ['i94yr', 'i94mon', 'dtadfile', 'epoch_start', 'status'\n",
    "                          'dtadfile', 'visapost', 'dtaddto', 'insnum', 'i94visa']\n",
    "\n",
    "immigration_fact_fields = ['cicid', 'status', 'adm_number', 'transport_mode', 'airport', \\\n",
    "                           'state', 'arrival_dt', 'departure_dt', 'airline', \\\n",
    "                           'fltno', 'visatype', 'age', 'gender', 'res_country', \\\n",
    "                           'cit_country', 'occup']\n",
    "\n",
    "\n",
    "immigration_facts = immigration_facts.select(immigration_fact_fields)\n",
    "immigration_facts.name = 'immigration_facts'\n",
    "immigration_facts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Exporting the data to Parquet\n",
    "We export the collected data in Parquet format after we have processed the cleaning steps above.\n",
    "Note that in this notebook we are only writing a sample of the immigration data. Dataframe `imm_full` contains all 3M rows we will write in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to write immigration_facts to location: temp/immigration_facts\n",
      "Done writing\n",
      "Trying to write dim_demographics to location: temp/dim_demographics\n",
      "Done writing\n",
      "Trying to write dim_airports to location: temp/dim_airports\n",
      "Done writing\n",
      "Trying to write dim_time to location: temp/dim_time\n",
      "Done writing\n",
      "Trying to write dim_status to location: temp/dim_status\n",
      "Done writing\n"
     ]
    }
   ],
   "source": [
    "def write_to_parquet(df=None, location='', name='', s3loc=''):\n",
    "    \"\"\" Writes provided dataframe content to parquet files\n",
    "        Use \"location\" variable to specify folder/sub-folders\n",
    "        and \"name\" for the parquet filename.\n",
    "        Optionally: provide an S3 bucket, the function will then\n",
    "        write to the S3 location\n",
    "        \n",
    "        Returns: Nothing\n",
    "    \"\"\"\n",
    "    # Put together an output location link\n",
    "    location = s3loc +location + name\n",
    "    try:\n",
    "        print('Trying to write {} to location: {}'.format(name, location))\n",
    "        df.write.parquet(location, mode='overwrite', compression='gzip')\n",
    "        print('Done writing')\n",
    "    except Exception as e:\n",
    "        print(datetime.now(), ': Writing output files failed: ', e)\n",
    "\n",
    "folder = 'temp/'\n",
    "s3loc = ''\n",
    "\n",
    "for table in table_list [immigration_facts, dim_demographics, dim_airports, dim_time, dim_status]:\n",
    "    write_to_parquet(df=table, location=folder, name=table.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
