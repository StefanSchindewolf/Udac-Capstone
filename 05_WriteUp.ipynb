{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Data ETL\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Introduction\n",
    "\n",
    "This notebook contains my comments on Step 5 of the Project Template. Here a short intro to the project:\n",
    "\n",
    "    Purpose of the project is to create an analytics dataset on US travellers that helps **FlyBy Salad**, a fast food franchise, to identify customer groups based on demographics, airport locations, travel status and dates.\n",
    "    \n",
    "    The project analyses the data using Pandas and Pyspark (Step 1 and 2). Then a Data Model is defined (Step 3).\n",
    "    \n",
    "    Then in Step 4 the project implements the data model in an ETL script that transforms immmigration data, airport and demographics information and saves this as Parquet files to the desired location.\n",
    "    \n",
    "    The data *could* also be loaded into a Redshift instance. The Parquet format includes the schema information for Redshift, so using the COPY command should work quite well. However this has not been implemented and tested and been taken out of scope.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Rationale\n",
    "\n",
    "The technology of choice here is Apache Spark because of it's scalability in processing large sets of data.\n",
    "\n",
    "While Airport and Demographic data could be handled even by Microsoft Excel the immigration data for April 2016 contains over 3 Million lines of data. This amount is too much and other months could contain even more data.\n",
    "\n",
    "In Step 1 I explored the data also using Pandas. But this only worked for a sample of the data. I wrote an explicit function to read only x-thousand lines from the SAS file in order to keep good runtime performance.\n",
    "\n",
    "Spark proved also a valid tool for assessing the data since PySpark implements basically a lot of Pandas' capabilities.\n",
    "\n",
    "For production use the ETL process needs to be scheduled and monitored. I recommend here Apache Airflow. Using other scheduling tools could be possible but this is beyond my knowledge.\n",
    "\n",
    "I developed all code in Jupyter notebooks as an \"IDE\". I tried to keep the code as simply and \"pluggable\" as possible. Then I copied everything into the \"etl.py\" and \"nb_helpers.py\" for commandline execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Spark scalability and performance improvements\n",
    "* The ETL script takes about 15 minutes to complete\n",
    "* Working with a sample of 1% results in a smaller immigration fact table, but the script's runtime does not significantly change\n",
    "* This means that a **100-times increase in data** had **no significant impact** on processing\n",
    "* Actually my test results were:\n",
    "\n",
    "| Sample size | QA Checks | Output to S3 | Runtime         |\n",
    "|-------------|-----------|--------------|-----------------|\n",
    "| 100% | YES | NO | 20 minutes |\n",
    "| 100% | NO | NO |  6:30 minutes |\n",
    "| 1% | YES | NO |  16 minutes |\n",
    "| 1% | NO | NO |  8 minutes |\n",
    "| 100% | NO | YES |  6:20 minutes |\n",
    "| 1% | NO | YES |  5:41 minutes |\n",
    "| 1% | YES | YES | 16:27 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Scheduling Updates\n",
    "\n",
    "* **Immigration Data** - since each file contains data for a complete month I would opt for a monthly update as well. This is of course under the assumption that the exact date and location of the source data production is known.\n",
    "\n",
    "* **Airport Data** - The data is updated daily, as the documentation states (https://datahub.io/core/airport-codes#automation). Since data quality proved rather poor here, with missing values and duplicates, I recommend a **daily update.**\n",
    "\n",
    "* **Demographics Set** - The data on opendatasoft is from 2017 (last update). So my recommendation would be to **not update at all**. Since demographics hardly change this seems acceptable.\n",
    "    * However since the United States Census Bureau has more datasets available it could also make sense to receive all historic data and then wait for **yearly updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Alternative Scenarios\n",
    "\n",
    "**The data was increased by 100x.**\n",
    "\n",
    "As explained above already only the airport data needs a daily update. The other sources change only on monthly and yearly basis.\n",
    "\n",
    "_However_ airport data cannot increase by 100x since this would require an equal construction of airports, presumably. The same affects the demographics data which _is already aggregated_. So an increase makes no sense here.\n",
    "\n",
    "Even if the size of this data increased I cannot see any performance issues coming up from the data model perspective.\n",
    "\n",
    "But IF Immigration data would have a size of 50 GB per month instead of (roughly) 500 MB we would face some issues:\n",
    "* Current processing time, including QA checks is 25 minutes\n",
    "* In theory a linear increase of runtime would mean, the ETL script would have a processing time of 50 hours (estimated 30 mins for current size)\n",
    "* This means (at minimum) a 100x **cost increase** to run it in AWS\n",
    "    * So not only for cost reasons does it make sense then to select a different approach, but also because of possible scaling issues in Spark\n",
    "* **Spark** tries to keep all data in memory so reading this data requires a large instance (e.g. `r4.4xlarge` which is memory-optimized)\n",
    "    * **Recommendation 1: choose a larger instance**\n",
    "* Another (probably more cost-effective way) is to split up the data (which also reqires compute power btw)\n",
    "    * **Recommendation 2: check if data can be splitted, e.g. by US State or Airport and then imported in parallel**\n",
    "    \n",
    "\n",
    "\n",
    "**The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    "\n",
    "I recommend to setup a daily data quality check and update of the data which runs during the night. Given the current runtime the update should start before 6:30 am.\n",
    "\n",
    "\n",
    "**The database needed to be accessed by 100+ people.\n",
    "\n",
    "Even if 100+ people needed access to the data I believe a large enough Redshift instance should be able to serve this requirement.\n",
    "\n",
    "For less customers, even a small Postgres instance could be sufficient.\n",
    "\n",
    "However if access should be granted it requires a \"form of access\" like a web gui - which needs to be taken into account when designing the overall system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
