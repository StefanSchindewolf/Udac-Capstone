{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Data ETL\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2 Explore and assess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Overview\n",
    "In this section we will explore the datasets chosen by **Flyby Salads** product management department using Apache Spark. We will idenfify and implement necessary transformations and finally export the data to S3 in parquet format.\n",
    "\n",
    "Subsections of this Notebook:\n",
    "* Preparations (imports, file locations, etc.)\n",
    "* Reading files using Spark\n",
    "* Data Exploration and description\n",
    "    * Dataframe schema analysis\n",
    "    * Data Quality analysis of the data\n",
    "    * Data Type transformation steps\n",
    "* Defining a new schema and storing it in JSON format\n",
    "* Logical Data exploration examples\n",
    "  (Analyzing table contents using PySpark to find more transformation actions)\n",
    "    * Creating an airport dimension table\n",
    "    * Linking airports to immigration data\n",
    "    * \n",
    "* Exporting the resulting tables to Parquet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note:**\n",
    "\n",
    "    * We want to keep track of transformation steps\n",
    "    * So if a required transformation is identified, it will be assigned a number like \"TFA-x\"\n",
    "    * Where \"TFA\" stands for transformation action (plus a number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from os.path import getsize\n",
    "from nb_helpers import summarize_data, get_sas_definitions, read_sas_in_chunks, \\\n",
    "                        read_csv_print, print_stat, non_iso_date_change, change_nullables\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s %(levelname)s \\t %(message)s ',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "log = logging.getLogger('log')\n",
    "\n",
    "# Improve view\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Reading data files into Spark Dataframes\n",
    "The next cell contains the file locations for each dataset, you may adjust it if required. Also note the configuration of the sample size which will improve performance while making some basic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Source file locations\n",
    "dem_file = 'us-cities-demographics.csv'\n",
    "imm_file = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "air_file = 'airport-codes_csv.csv'\n",
    "\n",
    "# Schema file locations\n",
    "air_schema_file = 'air_schema.json'\n",
    "dem_schema_file = 'dem_schema.json'\n",
    "imm_schema_file = 'imm_schema.json'\n",
    "\n",
    "\n",
    "# If working on a sample of the large immigration dataset, set your sample size here:\n",
    "sample_size = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "During my analysis I changed the schema of each dataset imported and based on the transformations done created a new schema definition. The definition is stored in a JSON file.\n",
    "\n",
    "The data will be **imported again** using those schema **at the end** of the \"Explore and Assess\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we setup the Spark Session and read all three files into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  3096313  lines from  ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "Reading  2891  lines from  us-cities-demographics.csv\n",
      "Reading  55075  lines from  airport-codes_csv.csv\n",
      "Operation runtime  0:00:42.500266\n"
     ]
    }
   ],
   "source": [
    "# Setup Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Read data from sources\n",
    "imm_df = spark.read \\\n",
    "                .format('com.github.saurfang.sas.spark') \\\n",
    "                .option(\"inferSchema\",\"true\") \\\n",
    "                .load(imm_file)\n",
    "imm_rows = imm_df.count()\n",
    "air_df = spark.read.csv(air_file, header=True)\n",
    "air_rows = air_df.count()\n",
    "dem_df = spark.read.csv(dem_file, header=True)\n",
    "dem_rows = dem_df.count()\n",
    "\n",
    "# List of our staging dataframes\n",
    "df_list = {'imm_df': imm_df, 'air_df': air_df, 'dem_df': dem_df}\n",
    "\n",
    "# Save a copy of initial total data, because we will work with a sample only in this notebook\n",
    "imm_full = imm_df\n",
    "\n",
    "# Output of results\n",
    "elapse = datetime.now() - start_time\n",
    "print('Reading ', imm_rows , ' lines from ', imm_file)\n",
    "print('Reading ', dem_rows, ' lines from ', dem_file)\n",
    "print('Reading ', air_rows, ' lines from ', air_file)\n",
    "print('Operation runtime ', elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Explore and Describe data\n",
    "This section contains the following subsections:\n",
    "* Spark schema analysis\n",
    "* Spark dataframe descriptions analysis\n",
    "\n",
    "Before we start this section we are naming each dataframe and reducing immigration data to a sample of the complete dataframe, keeping the full set in variable `imm_full` (the sample size being configured at the beginning of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_df = imm_full.sample(sample_size)\n",
    "imm_df.name = 'i94 immigration data'\n",
    "dem_df.name = 'city demographics'\n",
    "air_df.name = 'airport codes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Display schemas created automatically by Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imm_df.printSchema()\n",
    "air_df.printSchema()\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note** (TFA-1) Several columns in the i94 dataframe were infered as double while they should be of data type integer. Those types should be changed (see `cicid, i94yr,...`).\n",
    "\n",
    "**Note** (TFA-2) Several columns in airport and demographic data were infered as string where they should be either double or integer (see airport data, `elevation_ft` or demographic data `median_age`\n",
    "\n",
    "**Note** (TFA-3) Demographics data file was imported as a one-column file due to ';' being used as delimiter. We read the file again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dem_df = spark.read.csv(dem_file, header=True, sep=';')\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note**: (TFA-4) For easier handling we strip columns of spaces, apply lower case and replace inner spaces and '-' with underscore '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def trim_col_headers(df):\n",
    "    for col in df.columns:\n",
    "        changed_col = col.strip().lower().replace(' ', '_').replace('-', '_')\n",
    "        df = df.withColumnRenamed(col, changed_col)\n",
    "    return df\n",
    "\n",
    "dem_df = trim_col_headers(dem_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dataset descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Statistical summary on data\n",
    "The descriptive statistics were shown in Step 1 already, so we will skip this step here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Missing values per dataset\n",
    "First we are measuring the amount of empty cells per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-043bd3ee": {
       "style": "primary"
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get total amount of entries per column\n",
    "imm_col_count = imm_full.describe().filter(F.col('summary') == 'count')\n",
    "air_col_count = air_df.describe().filter(F.col('summary') == 'count')\n",
    "dem_col_count = dem_df.describe().filter(F.col('summary') == 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|summary|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|\n",
      "+-------+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|  count|100.0|100.0| 100.0| 100.0| 100.0|  100.0|  100.0|  100.0|   95.1|   95.4| 100.0|  100.0|100.0|   100.0|    39.2|  0.3|  100.0|   95.5|    0.0|   95.5|  100.0|  100.0|  86.6|   3.7|   97.3| 100.0| 99.4|   100.0|\n",
      "+-------+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "\n",
      "None\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|summary|ident| type| name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|  count|100.0|100.0|100.0|        87.3|    100.0|      100.0|     100.0|        89.7|    74.5|     16.7|      52.1|      100.0|\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n",
      "None\n",
      "+-------+-----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----+-----+\n",
      "|summary| city|state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code| race|count|\n",
      "+-------+-----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----+-----+\n",
      "|  count|100.0|100.0|     100.0|           99.9|             99.9|           100.0|              99.6|        99.6|                  99.4|     100.0|100.0|100.0|\n",
      "+-------+-----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Divide number of entries per column by total dataframe rows to get a percentage value\n",
    "for df, numrows in zip([imm_col_count, air_col_count, dem_col_count], \\\n",
    "                       [imm_rows, air_rows, dem_rows]):\n",
    "    for column in df.columns:\n",
    "        if column == 'summary':\n",
    "            next\n",
    "        else:\n",
    "            df = df.withColumn(column, F.col(column).cast('int'))\n",
    "            df = df.withColumn(column, F.bround((F.col(column) / numrows * 100), scale=1))\n",
    "    print('{}'.format(df.show()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Unique values per column\n",
    "We can count the number of unique values per dataframe or column using `dropDuplicates()` and `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique entries in table \"cicid\": 31020\n",
      "No. of unique entries in table \"i94yr\": 1\n",
      "No. of unique entries in table \"i94mon\": 1\n",
      "No. of unique entries in table \"i94cit\": 182\n",
      "No. of unique entries in table \"i94res\": 186\n",
      "No. of unique entries in table \"i94port\": 149\n",
      "No. of unique entries in table \"arrdate\": 30\n",
      "No. of unique entries in table \"i94mode\": 5\n",
      "No. of unique entries in table \"i94addr\": 101\n",
      "No. of unique entries in table \"depdate\": 173\n",
      "No. of unique entries in table \"i94bir\": 99\n",
      "No. of unique entries in table \"i94visa\": 3\n",
      "No. of unique entries in table \"count\": 1\n",
      "No. of unique entries in table \"dtadfile\": 65\n",
      "No. of unique entries in table \"visapost\": 257\n",
      "No. of unique entries in table \"occup\": 14\n",
      "No. of unique entries in table \"entdepa\": 13\n",
      "No. of unique entries in table \"entdepd\": 12\n",
      "No. of unique entries in table \"entdepu\": 2\n",
      "No. of unique entries in table \"matflag\": 2\n",
      "No. of unique entries in table \"biryear\": 99\n",
      "No. of unique entries in table \"dtaddto\": 265\n",
      "No. of unique entries in table \"gender\": 5\n",
      "No. of unique entries in table \"insnum\": 213\n",
      "No. of unique entries in table \"airline\": 169\n",
      "No. of unique entries in table \"admnum\": 31019\n",
      "No. of unique entries in table \"fltno\": 1965\n",
      "No. of unique entries in table \"visatype\": 15\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| 179.0|2016.0|   4.0| 103.0| 103.0|    WAS|20545.0|    1.0|     MD|20547.0|  43.0|    1.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1973.0|06292016|     M|  null|     OS|5.5425254633E10|00093|      WB|\n",
      "| 181.0|2016.0|   4.0| 103.0| 103.0|    WAS|20545.0|    1.0|     MD|20554.0|  29.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1987.0|06292016|  null|  null|     OS|5.5424291133E10|00093|      WT|\n",
      "| 277.0|2016.0|   4.0| 103.0| 103.0|    NYC|20545.0|    1.0|     NY|20563.0|  25.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1991.0|06292016|     M|  null|     OS|5.5438768033E10|00087|      WT|\n",
      "| 426.0|2016.0|   4.0| 103.0| 103.0|    MIA|20545.0|    1.0|     FL|20546.0|  39.0|    2.0|  1.0|20160401|    null| null|      G|      R|   null|      M| 1977.0|06292016|     M|  null|     OS|5.5427987133E10|00097|      WT|\n",
      "| 499.0|2016.0|   4.0| 103.0| 103.0|    FTL|20545.0|    1.0|     IL|20548.0|  55.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1961.0|06292016|  null|  null|     OS|5.5428463933E10|00065|      WB|\n",
      "| 592.0|2016.0|   4.0| 103.0| 103.0|    TAM|20545.0|    1.0|     FL|20547.0|  40.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1976.0|06292016|  null|  null|     LH|5.5450746133E10|00482|      WT|\n",
      "| 661.0|2016.0|   4.0| 103.0| 104.0|    SFR|20545.0|    1.0|     CA|20589.0|  36.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1980.0|06292016|     F|  null|     BA|5.5457843033E10|00287|      WT|\n",
      "| 720.0|2016.0|   4.0| 103.0| 438.0|    PHI|20545.0|    1.0|     FL|20553.0|  46.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1970.0|06292016|     F|  null|     AA|5.5439485933E10|00717|      WT|\n",
      "| 988.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     NY|20550.0|  16.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 2000.0|06292016|     F|  null|     UA|5.5421798833E10|00961|      WT|\n",
      "|1045.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     NY|20551.0|  48.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1968.0|06292016|     M|  null|     UA|5.5442351333E10|00055|      WT|\n",
      "|1056.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     NY|20551.0|  14.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 2002.0|06292016|     F|  null|     UA|5.5442402733E10|00055|      WT|\n",
      "|1064.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     NY|20552.0|  18.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1998.0|06292016|     M|  null|     UA|5.5433457933E10|02067|      WT|\n",
      "|1103.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     NY|20553.0|  33.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1983.0|06292016|     M|  null|     UA|5.5413642233E10|00056|      WT|\n",
      "|1381.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20550.0|  19.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1997.0|06292016|     F|  null|     SN|5.5420223033E10|01401|      WT|\n",
      "|1451.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20551.0|  17.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1999.0|06292016|     M|  null|     DL|5.5418483133E10|00049|      WT|\n",
      "|2045.0|2016.0|   4.0| 104.0| 104.0|    MIA|20545.0|    1.0|     FL|20554.0|  44.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1972.0|06292016|  null|  null|     LX|5.5421680533E10|00066|      WT|\n",
      "|2088.0|2016.0|   4.0| 105.0| 105.0|    SFR|20545.0|    1.0|     CA|20550.0|  55.0|    2.0|  1.0|20160401|     BEJ| null|      G|      O|   null|      M| 1961.0|09302016|     F|  null|     TK| 9.250717593E10|00079|      B2|\n",
      "|2181.0|2016.0|   4.0| 105.0| 105.0|    MIA|20545.0|    1.0|     FL|20555.0|  15.0|    2.0|  1.0|20160401|     SOF| null|      G|      O|   null|      M| 2001.0|09302016|     M|  null|     LH| 9.249016523E10|00460|      B2|\n",
      "|2189.0|2016.0|   4.0| 105.0| 105.0|    MIA|20545.0|    1.0|     FL|20546.0|  67.0|    2.0|  1.0|20160401|     SOF| null|      G|      R|   null|      M| 1949.0|09302016|     M|  null|     TK| 9.250735733E10|00077|      B2|\n",
      "|2191.0|2016.0|   4.0| 105.0| 105.0|    MIA|20545.0|    1.0|     FL|20546.0|  61.0|    2.0|  1.0|20160401|     SOF| null|      G|      R|   null|      M| 1955.0|09302016|     F|  null|     AZ| 9.248808173E10|00630|      B2|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "No. of unique entries in table \"ident\": 55075\n",
      "No. of unique entries in table \"type\": 7\n",
      "No. of unique entries in table \"name\": 52144\n",
      "No. of unique entries in table \"elevation_ft\": 5450\n",
      "No. of unique entries in table \"continent\": 7\n",
      "No. of unique entries in table \"iso_country\": 244\n",
      "No. of unique entries in table \"iso_region\": 2810\n",
      "No. of unique entries in table \"municipality\": 27134\n",
      "No. of unique entries in table \"gps_code\": 40851\n",
      "No. of unique entries in table \"iata_code\": 9043\n",
      "No. of unique entries in table \"local_code\": 27437\n",
      "No. of unique entries in table \"coordinates\": 54874\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "| 00AS|small_airport|      Fulton Airport|        1100|       NA|         US|     US-OK|        Alex|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      Cordes Airport|        3810|       NA|         US|     US-AZ|      Cordes|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|        3038|       NA|         US|     US-CA|     Barstow|    00CA|     null|      00CA|-116.888000488, 3...|\n",
      "| 00CL|small_airport| Williams Ag Airport|          87|       NA|         US|     US-CA|       Biggs|    00CL|     null|      00CL|-121.763427, 39.4...|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|        3350|       NA|         US|     US-CA| Pine Valley|    00CN|     null|      00CN|-116.4597417, 32....|\n",
      "| 00CO|       closed|          Cass Field|        4830|       NA|         US|     US-CO|  Briggsdale|    null|     null|      null|-104.344002, 40.6...|\n",
      "| 00FA|small_airport| Grass Patch Airport|          53|       NA|         US|     US-FL|    Bushnell|    00FA|     null|      00FA|-82.2190017700195...|\n",
      "| 00FD|     heliport|  Ringhaver Heliport|          25|       NA|         US|     US-FL|   Riverview|    00FD|     null|      00FD|-82.3453979492187...|\n",
      "| 00FL|small_airport|   River Oak Airport|          35|       NA|         US|     US-FL|  Okeechobee|    00FL|     null|      00FL|-80.9692001342773...|\n",
      "| 00GA|small_airport|    Lt World Airport|         700|       NA|         US|     US-GA|    Lithonia|    00GA|     null|      00GA|-84.0682983398437...|\n",
      "| 00GE|     heliport|    Caffrey Heliport|         957|       NA|         US|     US-GA|       Hiram|    00GE|     null|      00GE|-84.7339019775390...|\n",
      "| 00HI|     heliport|  Kaupulehu Heliport|          43|       NA|         US|     US-HI| Kailua/Kona|    00HI|     null|      00HI|-155.980233, 19.8...|\n",
      "| 00ID|small_airport|Delta Shores Airport|        2064|       NA|         US|     US-ID|  Clark Fork|    00ID|     null|      00ID|-116.213996887207...|\n",
      "| 00IG|small_airport|       Goltl Airport|        3359|       NA|         US|     US-KS|    McDonald|    00IG|     null|      00IG|-101.395994, 39.7...|\n",
      "| 00II|     heliport|Bailey Generation...|         600|       NA|         US|     US-IN|  Chesterton|    00II|     null|      00II|-87.122802734375,...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "No. of unique entries in table \"city\": 567\n",
      "No. of unique entries in table \"state\": 49\n",
      "No. of unique entries in table \"median_age\": 180\n",
      "No. of unique entries in table \"male_population\": 594\n",
      "No. of unique entries in table \"female_population\": 595\n",
      "No. of unique entries in table \"total_population\": 594\n",
      "No. of unique entries in table \"number_of_veterans\": 578\n",
      "No. of unique entries in table \"foreign_born\": 588\n",
      "No. of unique entries in table \"average_household_size\": 162\n",
      "No. of unique entries in table \"state_code\": 49\n",
      "No. of unique entries in table \"race\": 5\n",
      "No. of unique entries in table \"count\": 2785\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|            city|         state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|                race| count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White| 58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...| 24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White| 76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...|  1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...| 11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian| 32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian| 11060|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|American Indian a...|   998|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|         741270|           826172|         1567442|             61995|      205339|                  2.61|        PA|               Asian|122721|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...|  8791|\n",
      "|      Fort Myers|       Florida|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|               White| 50169|\n",
      "|      Pittsburgh|  Pennsylvania|      32.9|         149690|           154695|          304385|             17728|       28187|                  2.13|        PA|               White|208863|\n",
      "|          Laredo|         Texas|      28.8|         124305|           131484|          255789|              4921|       68427|                  3.66|        TX|American Indian a...|  1253|\n",
      "|        Berkeley|    California|      32.5|          60142|            60829|          120971|              3736|       25000|                  2.35|        CA|               Asian| 27089|\n",
      "|     Santa Clara|    California|      35.2|          63278|            62938|          126216|              4426|       52281|                  2.75|        CA|               White| 55847|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Count **distinct** entries per column and print\n",
    "for df, numrows in zip([imm_df, air_df, dem_df], [imm_rows, air_rows, dem_rows]):\n",
    "    for column in df.columns:\n",
    "        distinct_count = df.select(F.col(column)).distinct().count()\n",
    "        print('No. of unique entries in table \"{}\": {}'.format(column, distinct_count))\n",
    "    # print('{}'.format(df.show()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Transformations\n",
    "This section will contain the steps required to transform the datasets.\n",
    "\n",
    "From previous notes we now carry out some transformation actions.\n",
    "1. Double to Integer (TFA-1)\n",
    "1. String to numbers (TFA-2)\n",
    "1. SAS Date Type to date (TFA-5)\n",
    "1. Non-Iso Date Type to date (TFA-6)\n",
    "\n",
    "Note that TFA-3 and TFA-4 were already carried out in the previous chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Integer Type Transformation (TFA-1)\n",
    "The fields mentioned here are IntegerType but were infered as double, we are changing them to integer.\n",
    "\n",
    "In addition the string column `entdepa` will also be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, switched column cicid from DoubleType to IntegerType\n",
      "Done, switched column i94yr from DoubleType to IntegerType\n",
      "Done, switched column i94mon from DoubleType to IntegerType\n",
      "Done, switched column i94cit from DoubleType to IntegerType\n",
      "Done, switched column i94res from DoubleType to IntegerType\n",
      "Done, switched column arrdate from DoubleType to IntegerType\n",
      "Done, switched column depdate from DoubleType to IntegerType\n",
      "Done, switched column i94mode from DoubleType to IntegerType\n",
      "Done, switched column i94bir from DoubleType to IntegerType\n",
      "Done, switched column i94visa from DoubleType to IntegerType\n",
      "Done, switched column count from DoubleType to IntegerType\n",
      "Done, switched column biryear from DoubleType to IntegerType\n",
      "+-----+-----+------+------+------+-------+-------+-------+------+-------+-----+-------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|arrdate|depdate|i94mode|i94bir|i94visa|count|biryear|\n",
      "+-----+-----+------+------+------+-------+-------+-------+------+-------+-----+-------+\n",
      "|   21| 2016|     4|   101|   101|  20545|  20553|      1|    46|      2|    1|   1970|\n",
      "|   23| 2016|     4|   101|   101|  20545|  20671|      1|    52|      2|    1|   1964|\n",
      "|   47| 2016|     4|   101|   110|  20545|  20665|      1|    28|      3|    1|   1988|\n",
      "|  225| 2016|     4|   103|   103|  20545|  20550|      1|    58|      2|    1|   1958|\n",
      "|  507| 2016|     4|   103|   103|  20545|  20551|      1|    44|      1|    1|   1972|\n",
      "+-----+-----+------+------+------+-------+-------+-------+------+-------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Done, switched column admnum from DoubleType to LongType\n",
      "+-----------+\n",
      "|     admnum|\n",
      "+-----------+\n",
      "|92470796030|\n",
      "|92501394430|\n",
      "|92493977730|\n",
      "|55428558233|\n",
      "|55430114833|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "int_columns = ['cicid','i94yr', 'i94mon', 'i94cit', 'i94res', \\\n",
    "              'arrdate', 'depdate', 'i94mode', 'i94bir', \\\n",
    "              'i94visa', 'count', 'biryear']\n",
    "\n",
    "def change_type(df=None, fields=None, totype=None):\n",
    "    if ((totype is not None) and (fields is not None)):\n",
    "        for column in fields:\n",
    "            cur_type = df.schema[column]\n",
    "            df = df.withColumn(column, F.col(column).cast(totype))\n",
    "            new_type = df.schema[column]\n",
    "            print('Done, switched column {} from {} to {}' \\\n",
    "                  .format(column, cur_type.dataType, new_type.dataType))\n",
    "        print(df.select(fields).show(5))\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "imm_df = change_type(imm_df, int_fields, 'int')\n",
    "imm_df = change_type(imm_df, ['admnum'], 'bigint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### String to numbers (TFA-2)\n",
    "In dataframes for demographic and airport data no float types were infered. Instead we see floating point numbers imported as strings.\n",
    "Those will be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, switched column elevation_ft from StringType to DoubleType\n",
      "+------------+\n",
      "|elevation_ft|\n",
      "+------------+\n",
      "|        11.0|\n",
      "|      3435.0|\n",
      "|       450.0|\n",
      "|       820.0|\n",
      "|       237.0|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Airport data:\n",
    "float_columns = ['elevation_ft']\n",
    "int_columns = None\n",
    "\n",
    "air_df = change_type(air_df, float_columns, 'double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, switched column median_age from StringType to DoubleType\n",
      "Done, switched column average_household_size from StringType to DoubleType\n",
      "Done, switched column count from StringType to DoubleType\n",
      "+----------+----------------------+-------+\n",
      "|median_age|average_household_size|  count|\n",
      "+----------+----------------------+-------+\n",
      "|      33.8|                   2.6|25924.0|\n",
      "|      41.0|                  2.39|58723.0|\n",
      "|      38.5|                  2.58| 4759.0|\n",
      "|      34.5|                  3.18|24437.0|\n",
      "|      34.6|                  2.73|76402.0|\n",
      "+----------+----------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Done, switched column male_population from StringType to IntegerType\n",
      "Done, switched column female_population from StringType to IntegerType\n",
      "Done, switched column total_population from StringType to IntegerType\n",
      "Done, switched column number_of_veterans from StringType to IntegerType\n",
      "Done, switched column foreign_born from StringType to IntegerType\n",
      "+---------------+-----------------+----------------+------------------+------------+\n",
      "|male_population|female_population|total_population|number_of_veterans|foreign_born|\n",
      "+---------------+-----------------+----------------+------------------+------------+\n",
      "|          40601|            41862|           82463|              1562|       30908|\n",
      "|          44129|            49500|           93629|              4147|       32935|\n",
      "|          38040|            46799|           84839|              4819|        8229|\n",
      "|          88127|            87105|          175232|              5821|       33878|\n",
      "|         138040|           143873|          281913|              5829|       86253|\n",
      "+---------------+-----------------+----------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Demographic Data\n",
    "float_columns = ['median_age', 'average_household_size', 'count' ]\n",
    "int_columns = ['male_population', 'female_population', 'total_population', 'number_of_veterans', 'foreign_born']\n",
    "\n",
    "dem_df = change_type(dem_df, float_columns, 'double')\n",
    "dem_df = change_type(dem_df, int_columns, 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### SAS Date Type Transformations (TFA-5)\n",
    "The SAS formatted dates (e.v. \"20573.0\") should be changed to proper date types.\n",
    "Note we are not putting this into the nb_helpers.py because it' s a more exemplary kind of transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|   arrdate|   depdate|\n",
      "+----------+----------+\n",
      "|2016-04-01|2016-04-09|\n",
      "|2016-04-01|2016-08-05|\n",
      "|2016-04-01|2016-07-30|\n",
      "|2016-04-01|2016-04-06|\n",
      "|2016-04-01|2016-04-07|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sas_date_columns = ['arrdate', 'depdate']\n",
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "def sasdate_to_date(df=None, column_list=None):\n",
    "    # SAS has its own epoch, which we add temporarily as a column\n",
    "    df = df.withColumn('epoch_start', F.lit(\"01-01-1960 00:00:00\"))\n",
    "    df = df.withColumn('epoch_start', F.to_date(F.col('epoch_start'), \"dd-M-yyyy\"))\n",
    "    # Check each column and convert double to int, then add this int to epoch_start\n",
    "    for column in column_list:\n",
    "        df = df.withColumn(column, F.col(column).cast('int'))\n",
    "        stm = 'date_add(epoch_start, {})'.format(column)\n",
    "        df = df.withColumn(column, F.expr(stm))\n",
    "    return df\n",
    "\n",
    "imm_df = sasdate_to_date(imm_df, sas_date_columns)\n",
    "imm_df.select(sas_date_columns).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Non-Iso Date Type Transformations (TFA-6)\n",
    "Columns `dtadtto` and `dtadtofile` contain non-iso dateformats and will be converted into proper date types as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Changing SAS Date Type to Spark Datetype\n",
      "Done, switched column dtaddto from StringType to DateType\n",
      "Done, switched column dtadfile from StringType to DateType\n",
      "Here a view on some examples: \n",
      "+----------+\n",
      "|   dtaddto|\n",
      "+----------+\n",
      "|2016-09-30|\n",
      "|2016-09-30|\n",
      "|      null|\n",
      "|2016-06-29|\n",
      "|2016-06-29|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|  dtadfile|\n",
      "+----------+\n",
      "|2016-04-01|\n",
      "|2016-04-01|\n",
      "|2016-04-01|\n",
      "|2016-04-01|\n",
      "|2016-04-01|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "# if required, update the dictionary with a new column and a new pattern or change existing\n",
    "imm_non_iso_dates = {'dtaddto': 'MMddyyyy', 'dtadfile': 'yyyyMMdd'}    \n",
    "imm_df = non_iso_date_change(imm_df, imm_non_iso_dates)\n",
    "\n",
    "print('Here a view on some examples: ')\n",
    "print(imm_df.select('dtaddto').show(5), imm_df.select('dtadfile').show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Missing values and nullable setting (TFA-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Changing nullable setting\n",
    "Changing columns in the i94 dataset to `nullable = true` where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, Schema changed for  2  columns.\n"
     ]
    }
   ],
   "source": [
    "# Define fields which should not be empty\n",
    "not_null_fields = ['cicid', 'admnum']\n",
    "imm_df = imm_df.dropna(subset=not_null_fields)\n",
    "imm_df = change_nullables(session=spark, df=imm_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Demographic file did not have any missing values, so we set all fields to nullable = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, Schema changed for  12  columns.\n"
     ]
    }
   ],
   "source": [
    "not_null_fields = dem_df.columns\n",
    "dem_df = dem_df.na.drop(subset=not_null_fields)\n",
    "dem_df = change_nullables(session=spark, df=dem_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From airport data we want the items with iata code filled only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, Schema changed for  1  columns.\n"
     ]
    }
   ],
   "source": [
    "not_null_fields = ['iata_code']\n",
    "air_df = air_df.na.drop(subset=not_null_fields)\n",
    "air_df = change_nullables(session=spark, df=air_df, column_list=not_null_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Exporting a PySpark schema to JSON\n",
    "**(optional: re-importing data with updated schema from JSON)**\n",
    "Now that we have altered the dataframe schemas we export those schemas into JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write to Json file using a function\n",
    "def write_schema_json(df, schema_file):\n",
    "    import json\n",
    "    import sys\n",
    "    with open(schema_file, \"w\") as f:\n",
    "        json.dump(df.schema.jsonValue(), f)\n",
    "\n",
    "# Let's call that for each dataframe\n",
    "write_schema_json(imm_df, imm_schema_file)\n",
    "write_schema_json(dem_df, dem_schema_file)\n",
    "write_schema_json(air_df, air_schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**At this point we could read the JSON schema and apply it for importing data.**\n",
    "\n",
    "*However during my work I got a ClassCastException after importing the data on \".show()\" or any other actions.\n",
    "This is why the following cell is turned into markdown and exempted from execution*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#Save some lines by using a function to import a schema from JSON\n",
    "\n",
    "`def import_schema(schema_file):`   \n",
    "    `import json\n",
    "    schema_df = spark.sparkContext.wholeTextFiles(schema_file)\n",
    "    schema_content = schema_df.collect()[0][1]\n",
    "    schema_dict = json.loads(str(schema_content))\n",
    "    new_schema = T.StructType.fromJson(schema_dict)\n",
    "    return new_schema`\n",
    "\n",
    "#Now apply the schema for each\n",
    "`start_time = datetime.now()\n",
    "imm_df = spark.read.format('com.github.saurfang.sas.spark').load(path=imm_file, schema=import_schema(imm_schema_file))\n",
    "air_df = spark.read.csv(air_file, header=True, schema=import_schema(air_schema_file))\n",
    "dem_df = spark.read.csv(dem_file, header=True, sep=';', schema=import_schema(dem_schema_file))\n",
    "elapse = datetime.now() - start_time\n",
    "print('Reading ', imm_df.count(), ' lines from ', imm_file)\n",
    "print('Reading ', dem_df.count(), ' lines from ', dem_file)\n",
    "print('Reading ', air_df.count(), ' lines from ', air_file)\n",
    "print('Operation runtime ', elapse)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Resulting schema definitions\n",
    "In the sections above we have transformed specific values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = false)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: date (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: date (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- dtadfile: date (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: date (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: long (nullable = false)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- epoch_start: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: double (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = false)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = false)\n",
      " |-- state: string (nullable = false)\n",
      " |-- median_age: double (nullable = false)\n",
      " |-- male_population: integer (nullable = false)\n",
      " |-- female_population: integer (nullable = false)\n",
      " |-- total_population: integer (nullable = false)\n",
      " |-- number_of_veterans: integer (nullable = false)\n",
      " |-- foreign_born: integer (nullable = false)\n",
      " |-- average_household_size: double (nullable = false)\n",
      " |-- state_code: string (nullable = false)\n",
      " |-- race: string (nullable = false)\n",
      " |-- count: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imm_df.printSchema()\n",
    "air_df.printSchema()\n",
    "dem_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Logical exploration using PySpark\n",
    "In this chapter we will show some more explorative analysis that we help us understanding the data.\n",
    "\n",
    "**While doing this we will carry out more transformations. Those will be designated again as \"TFA-x\" to keep track of them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 01 - Cleaning airport data to get an airport dimension table (TFA-8)\n",
    "The airport codes table has over 50k entries. We can reduce it by filtering for *type airport* and for matching with immigration data we also need the *iata_code*.\n",
    "\n",
    "The resulting dataframe from the airport dataset will be kept as a transformed version of the table (trimmed of spaces in column `iata_code` and exported to Parquet later (dimension table \"airport\").\n",
    "\n",
    "The code below creates five examples of travellers and their corresponding airport information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully joined lines: 20489 \t\t\tJoin failed on 10530 lines\n",
      "+-------+----------+-------+---------+--------------------+------------+----------+-----------+------------------+-----------+\n",
      "|  cicid|   arrdate|airport|iata_code|                name|municipality|iso_region|iso_country|          latitude|  longitude|\n",
      "+-------+----------+-------+---------+--------------------+------------+----------+-----------+------------------+-----------+\n",
      "| 155458|2016-04-01|    BGM|      BGM|Greater Binghamto...|  Binghamton|     US-NY|         US|      -75.97979736|42.20869827|\n",
      "| 429152|2016-04-02|    FMY|      FMY|          Page Field|  Fort Myers|     US-FL|         US|-81.86329650879999|26.58659935|\n",
      "|4903447|2016-04-26|    FMY|      FMY|          Page Field|  Fort Myers|     US-FL|         US|-81.86329650879999|26.58659935|\n",
      "|5221677|2016-04-28|    FMY|      FMY|          Page Field|  Fort Myers|     US-FL|         US|-81.86329650879999|26.58659935|\n",
      "|3149638|2016-04-16|    FMY|      FMY|          Page Field|  Fort Myers|     US-FL|         US|-81.86329650879999|26.58659935|\n",
      "+-------+----------+-------+---------+--------------------+------------+----------+-----------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read all available airport codes from the airport dataframe\n",
    "airports_fields = ['iata_code', 'name', 'municipality', 'iso_region', \\\n",
    "                   'iso_country', 'coordinates']\n",
    "dim_airports = air_df.filter(air_df.iata_code.isNotNull()).select(airports_fields)\n",
    "\n",
    "# Split the coordinates column into latitude and longitude, remove spaces\n",
    "dim_airports = dim_airports.withColumn('latitude', F.split(dim_airports['coordinates'], ',').getItem(0)).withColumn('longitude', F.split(dim_airports['coordinates'], ',').getItem(1))\n",
    "dim_airports = dim_airports.withColumn('iata_code', F.trim(F.col('iata_code'))).withColumn('latitude', F.trim(F.col('latitude'))).withColumn('longitude', F.trim(F.col('longitude')))\n",
    "dim_airports = dim_airports.drop('coordinates')\n",
    "\n",
    "# TEST if data can be matched\n",
    "# Read some columns from immigration dataset including the airport codes, remove spaces\n",
    "imm_airports = imm_df.select(['cicid', 'arrdate', 'i94port']).withColumnRenamed('i94port', 'airport').distinct()\n",
    "imm_airports = imm_airports.withColumn('airport', F.trim(F.col('airport')))\n",
    "\n",
    "# Join with immigration dataset\n",
    "leftouter_set = imm_airports.join(dim_airports, dim_airports.iata_code == imm_airports.airport, 'leftouter')\n",
    "matching = leftouter_set.select(['iata_code']).filter(F.col('iata_code').isNotNull()).count()\n",
    "not_matching = leftouter_set.select(['iata_code']).filter(F.col('iata_code').isNull()).count()\n",
    "print('Successfully joined lines: {} \\t\\t\\tJoin failed on {} lines'.format(matching, (not_matching)))\n",
    "print(leftouter_set.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This catches **only about 65% of the entries** in our fact table.\n",
    "\n",
    "Now we could also try to match the city name in the SAS values list to the airport`municipality` column, but we consider this even worse.\n",
    "\n",
    "Therefore we check the airport codes in the immigration table which are not matching to column `i94port` and **count missed joins per airport**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|airport|count|\n",
      "+-------+-----+\n",
      "|    NYC| 4706|\n",
      "|    HHW| 1408|\n",
      "|    CHI| 1305|\n",
      "|    FTL|  948|\n",
      "|    LVG|  906|\n",
      "|    WAS|  742|\n",
      "|    SAI|  241|\n",
      "|    SAJ|   93|\n",
      "|    NOL|   47|\n",
      "|    XXX|   41|\n",
      "|    YGF|   29|\n",
      "|    X96|   22|\n",
      "|    PHU|   21|\n",
      "|    YHC|    7|\n",
      "|    INP|    3|\n",
      "|    JKM|    3|\n",
      "|    EPI|    2|\n",
      "|    MIL|    2|\n",
      "|    SSM|    2|\n",
      "|    PHR|    1|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get list of missing entries where airport code could not match\n",
    "not_matching = leftouter_set.select(['airport', 'iata_code']).filter(F.col('iata_code').isNull())\n",
    "# Count number of missing entries\n",
    "not_matching.groupBy('airport').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**This list sets a priority for cleaning up our airport dataframe or immigration data.**\n",
    "\n",
    "*Example: if we find out why \"NYC\" is not matching an entry of our airport CSV list we would have an additional 4.888 joins with our fact table(!)*\n",
    "\n",
    "One way to improve data quality on airports would be updating the airport code table with those missing codes and their corresponding city names. Another way would be to check if the airport code in the immigration data is correct at all.\n",
    "\n",
    "*At least for our example \"NYC\" it seems that this is not a proper iata_code. New York's airports have individual codes like \"LGA\" for LaGuardia Airport.*\n",
    "\n",
    "*This indicates that we should **correct the data in the immigration table** with the correct value. This requires however to **somehow derive the correct iata_code from other columns in the table**.\n",
    "\n",
    "I will take this data quality action **out of scope**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cicid',\n",
       " 'i94yr',\n",
       " 'i94mon',\n",
       " 'i94cit',\n",
       " 'i94res',\n",
       " 'i94port',\n",
       " 'arrdate',\n",
       " 'i94mode',\n",
       " 'i94addr',\n",
       " 'depdate',\n",
       " 'i94bir',\n",
       " 'i94visa',\n",
       " 'count',\n",
       " 'dtadfile',\n",
       " 'visapost',\n",
       " 'occup',\n",
       " 'entdepa',\n",
       " 'entdepd',\n",
       " 'entdepu',\n",
       " 'matflag',\n",
       " 'biryear',\n",
       " 'dtaddto',\n",
       " 'gender',\n",
       " 'insnum',\n",
       " 'airline',\n",
       " 'admnum',\n",
       " 'fltno',\n",
       " 'visatype',\n",
       " 'epoch_start']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 02 - Create status code dimension table (TFA-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------------+\n",
      "|entdepa|entdepd|entdepu|matflag| statusFlagId|\n",
      "+-------+-------+-------+-------+-------------+\n",
      "|      H|      O|      U|      M| 120259084288|\n",
      "|      Z|      I|      U|      M| 876173328384|\n",
      "|      G|      N|      U|      M| 919123001344|\n",
      "|      G|      O|      U|      M|1640677507072|\n",
      "+-------+-------+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract unique entries of status flags from immigration dataframe\n",
    "status_columns = ['entdepa', 'entdepd', 'entdepu', 'matflag']\n",
    "\n",
    "dim_status = imm_df.select(status_columns) \\\n",
    "                .dropna() \\\n",
    "                .dropDuplicates() \\\n",
    "                .withColumn(\"statusFlagId\", \\\n",
    "                            F.monotonically_increasing_id()) \n",
    "dim_status.name = 'dim_status'\n",
    "dim_status.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we check if we can match those to our airports dataframe using the short code in column `i94port`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 03 - Time Data dimension table (TFA-10)\n",
    "We create a dimension table from the provided immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the relevant fields where to collect date entries\n",
    "dim_time_columns = ['arrdate', 'depdate']\n",
    "\n",
    "# Collect all entries and remove duplicates\n",
    "dates_in_col = imm_df \\\n",
    "                .select(time_fields) \\\n",
    "                .dropDuplicates()\n",
    "\n",
    "# Concatenate the two columns (which needs another type conversion back to date)\n",
    "dim_time = dates_in_col.withColumn('datestamp', F.explode(F.col('arrdate')))\n",
    "dim_time.show()\n",
    "\n",
    "# Create a dataframe and fill it with all missing columns\n",
    "dim_time = spark.createDataFrame({'datestamp': dim_time})            \n",
    "dim_time.name = 'dim_time'\n",
    "dim_time = dim_time \\\n",
    "            .withColumn('day_of_week', F.dayofmonth('datestamp')) \\\n",
    "            .withColumn('week', F.weekofyear('datestamp')) \\\n",
    "            .withColumn('month', F.month('datestamp')) \\\n",
    "            .withColumn('year', F.year('datestamp')) \\\n",
    "            .dropDuplicates()\n",
    "\n",
    "#Output\n",
    "dim_time.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 04 - Demographics dimension table (TFA-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_columns = dem_df.columns\n",
    "dim_demographics = dem_df.select(demographics_columns).dropDuplicates()\n",
    "dim_demographics.name = 'dim_demographics'\n",
    "dim_demographics.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example 05 - Immigration Fact Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_fact_fields = ['cicid']\n",
    "immigration_facts = imm_df \\\n",
    "                    .select(immigration_fact_fields) \\\n",
    "                    .withColumnRenamed('arrdate','arrival_dt') \\\n",
    "                    .withColumnRenamed('depdate','departure_dt') \\\n",
    "immigration_facts.name = 'immigration_facts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Exporting the data to Parquet\n",
    "We export the collected data in Parquet format after we have processed the cleaning steps above.\n",
    "Note that in this notebook we are only writing a sample of the immigration data. Dataframe `imm_full` contains all 3M rows we will write in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_facts.write.parquet(\"immigration_facts_data\", mode='overwrite', compression='gzip')\n",
    "dim_demographics.write.parquet('dim_demographics_data', mode='overwrite', compression='gzip')\n",
    "dim_airports.write.parquet('dim_airports_data', mode='overwrite', compression='gzip')\n",
    "dim_time.write.parquet('dim_time_data', mode='overwrite', compression='gzip')\n",
    "dim_status.write.parquet('dim_status_data', mode='overwrite', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
