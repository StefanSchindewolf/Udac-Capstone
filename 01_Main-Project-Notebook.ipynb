{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s %(levelname)s \\t %(message)s ',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "log = logging.getLogger('log')\n",
    "\n",
    "# Improve view\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\n",
    "def quantify_data(df, type_choice=None, examples=10):\n",
    "    \"\"\" DOC STRING\"\"\"\n",
    "    # Set number of examples to be printed per value\n",
    "    # examples = 10\n",
    "    \n",
    "    # If type_choice is set, only the dtypes provided will be analysed\n",
    "    if type_choice is None:\n",
    "        # If not set, we simple analyze numeric and string data and\n",
    "        # print the result\n",
    "        type_choice = ['all']\n",
    "        print('Running Data Quantifier with parameter: ', ', '.join(type_choice),\\\n",
    "             ' and example threshhold is ', examples)\n",
    "    else:\n",
    "        print('Running Data Quantifier with parameter: ',  ', '.join(type_choice),\\\n",
    "             ' and example threshhold is ', examples)\n",
    "    \n",
    "    # Analysis section\n",
    "    if (('all' in type_choice) or ('numbers' in type_choice)):\n",
    "        # NUMERIC DATA ANALYSIS\n",
    "        sub_df = df.select_dtypes(exclude=['object'])\n",
    "        print('\\nQuantifying NUMERIC data types in columns:\\n',  ', '.join(sub_df.columns), '\\n')\n",
    "        # Get descriptive statistics\n",
    "        stat_df = sub_df.describe()\n",
    "        # Count missing values per column\n",
    "        miss_df = pd.DataFrame.from_dict({'Missing': sub_df.isna().sum()})\n",
    "        #miss_df = miss_df['Missing'].astype(int)\n",
    "        #mis_val_cols = miss_df.loc[miss_df['Missing'] > 0].columns\n",
    "        mis_val_cols = miss_df[miss_df > 0].dropna().index\n",
    "        # Count unique values per column\n",
    "        uniq_df = pd.DataFrame.from_dict({'Unique': sub_df.nunique()})\n",
    "        #uniq_df = uniq_df['Unique'].astype(int)\n",
    "        # Get list of example values for columns which have less than x unique values\n",
    "        uni_val_cols = uniq_df[uniq_df <= examples].dropna().index\n",
    "        uniq_df = uniq_df.transpose()\n",
    "        miss_df = miss_df.transpose()\n",
    "        stat_df = pd.concat([stat_df, uniq_df, miss_df])\n",
    "        display(stat_df)\n",
    "        print('Columns with missing values: ', ','.join(mis_val_cols), '\\n')\n",
    "        for unique_value_column in uni_val_cols:\n",
    "            unique_values = df[unique_value_column].drop_duplicates()\n",
    "            msg = 'Unique values in column \\'{}\\': \\n'.format(unique_value_column)\n",
    "            print(msg, unique_values.values, '\\n')\n",
    "        #print('Columns with missing values: ', ','.join(mis_val_cols))\n",
    "\n",
    "    if (('all' in type_choice) or ('object' in type_choice)):\n",
    "        # STRING DATA ANALYSIS\n",
    "        sub_df = df.select_dtypes(exclude=['float64'])\n",
    "        print('\\nQuantifying NON-NUMERIC data types in columns:\\n',  ', '.join(sub_df.columns))\n",
    "        stat_df = pd.DataFrame.from_dict(data=dict(sub_df.dtypes), orient='index', columns=['Datatype'])\n",
    "        stat_df['Lines'] = len(df)\n",
    "        stat_df['Non-Null'] = df.count()\n",
    "        stat_df['NaN'] = df.isna().sum()\n",
    "        stat_df['Fill-%'] = df.count() / len(df) *100\n",
    "        stat_df['Unique'] = df.nunique()\n",
    "        stat_df['Uniq-%'] = stat_df['Unique'] / stat_df['Lines'] *100\n",
    "        mis_val_cols = list(stat_df.loc[stat_df['Fill-%'] < 100].index)\n",
    "        uni_val_cols = list(stat_df.loc[stat_df['Unique'] <= examples].index)\n",
    "        display(stat_df.transpose())\n",
    "        print('Columns with missing values: ', ','.join(mis_val_cols), '\\n')\n",
    "        for unique_value_column in uni_val_cols:\n",
    "            unique_values = df[unique_value_column].drop_duplicates()\n",
    "            msg = 'Unique values in column \\'{}\\': \\n'.format(unique_value_column)\n",
    "            print(msg, unique_values.values, '\\n')\n",
    "    print(\"\\n\\nData Quantification Done\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 1 - Scoping and Data Gathering\n",
    "**Task: Scope the Project and Gather Data**\n",
    "\n",
    "*Identify and gather the data you'll be using for your project (at least two sources and more than 1 million rows). See Project Resources for ideas of what data you can use.*\n",
    "\n",
    "*Explain what end use cases you'd like to prepare the data for (e.g., analytics table, app back-end, source-of-truth database, etc.)*\n",
    "\n",
    "\n",
    "## Step 1a - General Scope and Data Gathering Description\n",
    "The Udacity provided datasets for the Capstone Project include:\n",
    "* I94 Immigration data from 2016 provided by U.S. Customs and Border Protection agency\n",
    "* World Temperature Data\n",
    "* U.S. cities demographic data\n",
    "* An airport code table\n",
    "\n",
    "Each dataset has been collected at least once for assessment. The findings are included in the following chapters of this notebook, even if the dataset is not used in Step 2.\n",
    "\n",
    "Regarding the scope itself the following findings are relevant:\n",
    "* **I94 Immigration data** is considered **in scope** regarding the following analytical tasks:\n",
    "    * Develop a scalable automated extraction procedure using Spark Data Lake\n",
    "    * Load and Transform the data into fact and dimension tables\n",
    "    * Develop Airflow routines to manage the process\n",
    "* **Airport Codes** are considered **in scope** and will be used\n",
    "    * to enrich the immigration dataset with complete and updated values\n",
    "* **World Temperature data** is considered **out of scope** since no analytics questions for this dataset in conjunction with immigration data could be identified _and_ the datasets' time periods do not overlap\n",
    "* **Demographic data** is considered\n",
    "\n",
    "**Approach to describe and gather data**\n",
    "\n",
    "Descriptions for each dataset will be given in the sections below. Each description shall include:\n",
    "1. A first read of the dataset using Python and Pandas default methods\n",
    "1. \"First Impression\" notes about the extracted data\n",
    "1. Analysis of dataset documentation, enclosed data dictionaries, etc.\n",
    "1. Findings about Data Meaning, Quality, possible relationsships and definitions for\n",
    "    1. Numeric columns (including missing values, uniqueness and descriptive statistics)\n",
    "    1. Non-numeric columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1b - I94 Dataset of U.S. Customs and Border Protection department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### A - I94 Immigration Dataset Description\n",
    "The dataset provided contains immigration data provided by US immigration authorities. Data is collected via form **I94** and contains data about people travelling from and to the US on people who are either **non United States citizens** or **lawful permanent residents** in the US.\n",
    "\n",
    "    “Form I-94, the Arrival-Departure Record Card, is a form used by the U.S. Customs and Border Protection (CBP) intended to keep track of the arrival and departure to/from the United States of people who are not United States citizens or lawful permanent residents (with the exception of those who are entering using the Visa Waiver Program or Compact of Free Association, using Border Crossing Cards, re-entering via automatic visa revalidation, or entering temporarily as crew members)” (https://en.wikipedia.org/wiki/Form_I-94)\n",
    "\n",
    "An overview of this dataset is also outlined [here] (https://travel.trade.gov/research/programs/i94/description.asp)\n",
    "\n",
    "Data files and formats:\n",
    "- Data files are stored in SAS (proprietary?) sas7bdat format\n",
    "- Per year a folder exists\n",
    "- Per month a file exists (~500 GB)\n",
    "\n",
    "Description file:\n",
    "- A description file for the fields was included, named *I94_SAS_Labels_Descriptions.SAS*\n",
    "- The file contains field descriptions for each column\n",
    "- And it contains value constraints for some columns, namely: *i94cnty, i94port, i94mode, i94addr*\n",
    "\n",
    "### B - I94 Immigration Data Data Gathering and first read\n",
    "\n",
    "As Pandas has a method to import SAS data we will be using this mechanism. The following code will read a defined number of lines only due to performance reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START reading SAS file  ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "\t\t\tImporting lines from 1 to 2000 of total 6000 lines\n",
      "\t\t\tImporting lines from 2001 to 4000 of total 6000 lines\n",
      "\t\t\tImporting lines from 4001 to 6000 of total 6000 lines\n",
      "STOP reading SAS files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130811</td>\n",
       "      <td>SEO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa entdepd entdepu matflag  biryear   dtaddto gender insnum airline        admnum  fltno visatype\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN      NaN    37.0      2.0    1.0       NaN      NaN   NaN       T     NaN       U     NaN   1979.0  10282016    NaN    NaN     NaN  1.897628e+09    NaN       B2\n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL      NaN    25.0      3.0    1.0  20130811      SEO   NaN       G     NaN       Y     NaN   1991.0       D/S      M    NaN     NaN  3.736796e+09  00296       F1\n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI  20691.0    55.0      2.0    1.0  20160401      NaN   NaN       T       O     NaN       M   1961.0  09302016      M    NaN      OS  6.666432e+08     93       B2\n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA  20567.0    28.0      2.0    1.0  20160401      NaN   NaN       O       O     NaN       M   1988.0  09302016    NaN    NaN      AA  9.246846e+10  00199       B2\n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA  20567.0     4.0      2.0    1.0  20160401      NaN   NaN       O       O     NaN       M   2012.0  09302016    NaN    NaN      AA  9.246846e+10  00199       B2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data using read_sas() method\n",
    "sas_file =  '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "sas_df = pd.DataFrame()\n",
    "lines_imported = 0\n",
    "max_lines=6000     # Set the desired line number here\n",
    "for_lines=2000     # Set the desired lines for each cycle here\n",
    "\n",
    "print('START reading SAS file ', sas_file)\n",
    "# The method _read_sas()_ will read the files in chunks\n",
    "for chunk in pd.read_sas(sas_file, 'sas7bdat', encoding=\"ISO-8859-1\", chunksize=for_lines):\n",
    "    last_lines = lines_imported + 1\n",
    "    lines_imported = lines_imported + len(chunk)\n",
    "    sas_df = sas_df.append(chunk)\n",
    "    print('\\t\\t\\tImporting lines from {} to {} of total {} lines'.format(last_lines, lines_imported, max_lines))\n",
    "    if lines_imported >= max_lines:\n",
    "        print('STOP reading SAS files')\n",
    "        break\n",
    "\n",
    "sas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Notes and Findings on First Read:**\n",
    "* (N) We are not importing everything here, since the files amount to about 6GB in total\n",
    "* (N) Using \"chunksize\" parameter and then breaking from the loop, so that we have handy **2.000 lines\n",
    "* (F) In total 28 columns exist, 15 columns contain strings (object type) and 13 contain numbers (float64 type)\n",
    "* At first sight one can already spot unfamiliar date columns (arrdate, depdate, etc.) with various datatypes\n",
    "* Several rows have missing values\n",
    "* Some columns contain obviously integer values but float64 was assigned\n",
    "* Some categorical columns seem to exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### C - Documentation Analysis\n",
    "\n",
    "The workspace contains a field description file for the dataset named `I94_SAS_Labels_Descriptions.SAS`\n",
    "\n",
    "The file seems pretty well structured, so I wrote a quick parser to automatically check the description file (see [SAS-Description-Parser](https://r766466c839826xjupyterlnnfq3jud.udacity-student-workspaces.com/lab/tree/SAS-Description-Parser.ipynb) for further details).\n",
    "\n",
    "**Definitions**\n",
    "\n",
    "| **Variable name** | **Data Type** | **Description** |\n",
    "|---------------|---------------|---------------|\n",
    "| i94yr | float64 | 4 digit year |\n",
    "| i94mon | float64 | Numeric month |\n",
    "| i94cit | float64 | This format shows all the valid and invalid codes for processing |\n",
    "| i94res | float64 | This format shows all the valid and invalid codes for processing |\n",
    "| i94port | object | This format shows all the valid and invalid codes for processing |\n",
    "| arrdate | float64 | is the Arrival Date in the USA. It is a SAS date numeric field that apermament format has not been applied.  Please apply whichever date formatpermament format has not been applied.  Please apply whichever date format |\n",
    "| i94mode | float64 | There are missing values as well as not reported (9) |\n",
    "| i94addr | object | There is lots of invalid codes in this variable and the list belowThere is lots of invalid codes in this variable and the list below |\n",
    "| depdate | float64 | is the Departure Date from the USA. It is a SAS date numeric field thata permament format has not been applied.  Please apply whichever date formata permament format has not been applied.  Please apply whichever date format |\n",
    "| i94bir | float64 | Age of Respondent in Years |\n",
    "| i94visa | float64 | Visa codes collapsed into three categories:1 = Business2 = Pleasure3 = Student*/ |\n",
    "| count | float64 | Used for summary statistics |\n",
    "| dtadfile | object | Character Date Field |\n",
    "| visapost | object | Department of State where where Visa was issued |\n",
    "| occup | object | Occupation that will be performed in U.S. |\n",
    "| entdepa | object | Arrival Flag |\n",
    "| entdepd | object | Departure Flag |\n",
    "| entdepu | object | Update Flag |\n",
    "| matflag | object | Match flag |\n",
    "| biryear | float64 | 4 digit year of birth |\n",
    "| dtaddto | object | Character Date Field |\n",
    "| gender | object | Non |\n",
    "| insnum | object | INS number |\n",
    "| airline | object | Airline used to arrive in U.S. |\n",
    "| admnum | float64 | Admission Number |\n",
    "| fltno | object | Flight number of Airline used to arrive in U.S. |\n",
    "| visatype | object | Class of admission legally admitting the non |\n",
    "\n",
    "**Findings on value constraints**\n",
    "\n",
    "Columns `i94cnty, i94port, i94mode, i94addr` have value constraints (lists with allowed entry values) which are outlined here:\n",
    "* `i94cnty` contains country short codes and their corresponding state names\n",
    "* `i94port`contains port/airport codes from various cities\n",
    "    * There doesn't seem to be a specific selection criteria\n",
    "    * Although most of the codes are cities in the US we also see city codes from Europe and Asia\n",
    "* `i94mode` is a code for the way of travelling (by Air, by Sea or by Land) or unknown\n",
    "* `i94addr`is a code for the state in which this immigrants temporary address is located (aka \"First Intended Address\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### D - Analysis of numeric columns\n",
    "The Pandas describe() function creates a basic set of descriptive statistics for each numeric column in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Data Quantifier with parameter:  numbers\n",
      "\n",
      "Quantifying NUMERIC data types in columns:\n",
      " cicid, i94yr, i94mon, i94cit, i94res, arrdate, i94mode, depdate, i94bir, i94visa, count, biryear, admnum\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>biryear</th>\n",
       "      <th>admnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1927.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2.000000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1146.768000</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>104.16250</td>\n",
       "      <td>111.666000</td>\n",
       "      <td>20545.017000</td>\n",
       "      <td>1.006003</td>\n",
       "      <td>20556.293202</td>\n",
       "      <td>38.968000</td>\n",
       "      <td>1.9015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1977.032000</td>\n",
       "      <td>5.909467e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>650.874837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.60072</td>\n",
       "      <td>54.716217</td>\n",
       "      <td>0.640247</td>\n",
       "      <td>0.077265</td>\n",
       "      <td>16.081589</td>\n",
       "      <td>17.266993</td>\n",
       "      <td>0.3144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.266993</td>\n",
       "      <td>1.507924e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>20545.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20546.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1929.000000</td>\n",
       "      <td>6.644910e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>565.750000</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>103.00000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>20545.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20550.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1964.000000</td>\n",
       "      <td>5.542381e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1159.500000</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>104.00000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>20545.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20552.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1977.000000</td>\n",
       "      <td>5.543718e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1678.250000</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>104.00000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>20545.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20558.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1990.000000</td>\n",
       "      <td>5.545536e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2302.000000</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.00000</td>\n",
       "      <td>692.000000</td>\n",
       "      <td>20573.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20704.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>9.251651e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unique</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>2.000000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cicid   i94yr  i94mon      i94cit       i94res       arrdate      i94mode       depdate       i94bir    i94visa   count      biryear        admnum\n",
       "count   2000.000000  2000.0  2000.0  2000.00000  2000.000000   2000.000000  1999.000000   1927.000000  2000.000000  2000.0000  2000.0  2000.000000  2.000000e+03\n",
       "mean    1146.768000  2016.0     4.0   104.16250   111.666000  20545.017000     1.006003  20556.293202    38.968000     1.9015     1.0  1977.032000  5.909467e+10\n",
       "std      650.874837     0.0     0.0    13.60072    54.716217      0.640247     0.077265     16.081589    17.266993     0.3144     0.0    17.266993  1.507924e+10\n",
       "min        6.000000  2016.0     4.0   101.00000   101.000000  20545.000000     1.000000  20546.000000     0.000000     1.0000     1.0  1929.000000  6.644910e+08\n",
       "25%      565.750000  2016.0     4.0   103.00000   103.000000  20545.000000     1.000000  20550.000000    26.000000     2.0000     1.0  1964.000000  5.542381e+10\n",
       "50%     1159.500000  2016.0     4.0   104.00000   104.000000  20545.000000     1.000000  20552.000000    39.000000     2.0000     1.0  1977.000000  5.543718e+10\n",
       "75%     1678.250000  2016.0     4.0   104.00000   104.000000  20545.000000     1.000000  20558.000000    52.000000     2.0000     1.0  1990.000000  5.545536e+10\n",
       "max     2302.000000  2016.0     4.0   692.00000   692.000000  20573.000000     2.000000  20704.000000    87.000000     3.0000     1.0  2016.000000  9.251651e+10\n",
       "Unique  2000.000000     1.0     1.0     8.00000    36.000000      3.000000     2.000000     80.000000    83.000000     3.0000     1.0    83.000000  2.000000e+03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Data Quantification Done\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantify_data(sas_df, ['numbers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Summary on numeric data:**\n",
    "\n",
    "| **Column Name** | **Commends and findings:**                  | **Recommended action items** |\n",
    "|------------|--------------------------------------------------|----------------------------|\n",
    "| `cicid` |is unique for all 2.000 lines (check `len(sas_df['cicid'].unique())`) and appears to be the primary key for each record | |\n",
    "| | The following columns appear to indicate datetime related values: |\n",
    "| `i94yr` |indicating the year the I94 form was filled and 'i94mon' indicating the month | |\n",
    "| `arrdate` |is the immigrants arrival date | |\n",
    "| `depdate` |the date of the immigrants (planned) departure | |\n",
    "| `dtadfile` |is the date on which the form was entered into the database | |\n",
    "| `dtaddto` |is the date the immigrant is admissioned to stay in the US | |\n",
    "| `i94mode` | has already been identified as a category variable, the integers here are just codes indicating if the immigrant travelled by Land, Air or Sea (or unknown) | |\n",
    "| `i94visa` | was not identified correctly by my parser it seems, it has value constraints (* 1 = Business, 2 = Pleasure,3 = Student)  | |\n",
    "| `i94cit` and `i94res` | are again not numeric but indicate the immigrant's countries of citizenship (\"cit\") and residence (res) | |\n",
    "|`admnum` | is the admission number | |\n",
    "|`i94bir` |appears to be the immigrant's age at the time of admission (in other words it's the time delta between `i94yr`and `biryear` | |\n",
    "| `biryear` |marks the immigrants birthyear | |\n",
    "| `count` |is for statistical purposes according to the description | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### E - Quantitative Analysis of non-numeric data\n",
    "Measuring the number of NaN entries and unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Data Quantifier with parameter:  object  and example threshhold is  10\n",
      "\n",
      "Quantifying NON-NUMERIC data types in columns:\n",
      " i94port, i94addr, dtadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, dtaddto, gender, insnum, airline, fltno, visatype\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i94port</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Datatype</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lines</th>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-Null</th>\n",
       "      <td>6000</td>\n",
       "      <td>5846</td>\n",
       "      <td>5999</td>\n",
       "      <td>675</td>\n",
       "      <td>3</td>\n",
       "      <td>6000</td>\n",
       "      <td>5821</td>\n",
       "      <td>2</td>\n",
       "      <td>5821</td>\n",
       "      <td>5999</td>\n",
       "      <td>5247</td>\n",
       "      <td>0</td>\n",
       "      <td>5998</td>\n",
       "      <td>5999</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>5325</td>\n",
       "      <td>5997</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>5998</td>\n",
       "      <td>179</td>\n",
       "      <td>1</td>\n",
       "      <td>753</td>\n",
       "      <td>6000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fill-%</th>\n",
       "      <td>100</td>\n",
       "      <td>97.4333</td>\n",
       "      <td>99.9833</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>100</td>\n",
       "      <td>97.0167</td>\n",
       "      <td>0.0333333</td>\n",
       "      <td>97.0167</td>\n",
       "      <td>99.9833</td>\n",
       "      <td>87.45</td>\n",
       "      <td>0</td>\n",
       "      <td>99.9667</td>\n",
       "      <td>99.9833</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unique</th>\n",
       "      <td>51</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>455</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uniq-%</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.0333333</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0333333</td>\n",
       "      <td>0.0166667</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.0333333</td>\n",
       "      <td>0</td>\n",
       "      <td>1.38333</td>\n",
       "      <td>7.58333</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         i94port   i94addr   dtadfile visapost   occup entdepa   entdepd    entdepu    matflag  dtaddto     gender  insnum  airline    fltno  visatype\n",
       "Datatype  object    object     object   object  object  object    object     object     object   object     object  object   object   object    object\n",
       "Lines       6000      6000       6000     6000    6000    6000      6000       6000       6000     6000       6000    6000     6000     6000      6000\n",
       "Non-Null    6000      5846       5999      675       3    6000      5821          2       5821     5999       5247       0     5998     5999      6000\n",
       "NaN            0       154          1     5325    5997       0       179       5998        179        1        753    6000        2        1         0\n",
       "Fill-%       100   97.4333    99.9833    11.25    0.05     100   97.0167  0.0333333    97.0167  99.9833      87.45       0  99.9667  99.9833       100\n",
       "Unique        51        56          2       45       3       6         8          2          1       33          2       0       83      455         8\n",
       "Uniq-%      0.85  0.933333  0.0333333     0.75    0.05     0.1  0.133333  0.0333333  0.0166667     0.55  0.0333333       0  1.38333  7.58333  0.133333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:  i94addr,dtadfile,visapost,occup,entdepd,entdepu,matflag,dtaddto,gender,insnum,airline,fltno \n",
      "\n",
      "Unique values in column 'dtadfile': \n",
      " [nan '20130811' '20160401'] \n",
      "\n",
      "Unique values in column 'occup': \n",
      " [nan 'ELT' 'PHS' 'EXA'] \n",
      "\n",
      "Unique values in column 'entdepa': \n",
      " ['T' 'G' 'O' 'H' 'U' 'B'] \n",
      "\n",
      "Unique values in column 'entdepd': \n",
      " [nan 'O' 'K' 'I' 'Q' 'R' 'N' 'M' 'J'] \n",
      "\n",
      "Unique values in column 'entdepu': \n",
      " ['U' 'Y' nan] \n",
      "\n",
      "Unique values in column 'matflag': \n",
      " [nan 'M'] \n",
      "\n",
      "Unique values in column 'gender': \n",
      " [nan 'M' 'F'] \n",
      "\n",
      "Unique values in column 'insnum': \n",
      " [nan] \n",
      "\n",
      "Unique values in column 'visatype': \n",
      " ['B2' 'F1' 'B1' 'WT' 'WB' 'E2' 'I' 'F2'] \n",
      "\n",
      "\n",
      "\n",
      "Data Quantification Done\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantify_data(sas_df, ['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Summary on non-numeric data:**\n",
    "\n",
    "| **Column Name** | **Commends and findings:**                  | **Recommended action items** |\n",
    "|------------|--------------------------------------------------|----------------------------|\n",
    "| several | columns have missing values (i94addr,dtadfile,visapost,occup,entdepd,entdepu,matflag,dtaddto,gender,insnum,airline,fltno) | |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fbfd84abd3ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msas_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_sas_definitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0msasdefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sas_definitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I94_SAS_Labels_Descriptions.SAS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msasdefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Note: IN SAS dates are a special case of numeric values. Each day is assigned a specific numeric value starting from 1st January 1960.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'dtypes'"
     ]
    }
   ],
   "source": [
    "new_df = sas_df.copy()\n",
    "\n",
    "to_datetime_cols = ['arrdate', 'depdate']\n",
    "\n",
    "to_int_cols = ['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res',\\\n",
    "               'i94port', 'i94mode', 'i94bir', 'i94visa', 'count',\\\n",
    "               'biryear'\n",
    "              ]\n",
    "\n",
    "to_string_cols = []\n",
    "\n",
    "sas_df_category_columns = ['i94mode', 'gender', 'occup']\n",
    "\n",
    "sas_df_type = {\n",
    "'cicid':'int',\n",
    "'i94yr':'int',\n",
    "'i94mon':'int',\n",
    "'i94cit':'int',\n",
    "'i94res':'int',\n",
    "'i94port':'object',\n",
    "'arrdate':'int',\n",
    "'i94mode':'object',\n",
    "'i94addr':'object',\n",
    "'depdate':'int',\n",
    "'i94bir':'float64',\n",
    "'i94visa':'float64',\n",
    "'count':'int',\n",
    "'dtadfile':'object',\n",
    "'visapost':'object',\n",
    "'occup':'object',\n",
    "'entdepa':'object',\n",
    "'entdepd':'object',\n",
    "'entdepu':'object',\n",
    "'matflag':'object',\n",
    "'biryear':'int',\n",
    "'dtaddto':'object',\n",
    "'gender':'object',\n",
    "'insnum':'object',\n",
    "'airline':'object',\n",
    "'admnum':'float64',\n",
    "'fltno':'object',\n",
    "'visatype':'category'\n",
    "}\n",
    "\n",
    "# Import SAS definition file\n",
    "from sas_helpers import get_sas_definitions\n",
    "sasdefs = get_sas_definitions('I94_SAS_Labels_Descriptions.SAS')\n",
    "\n",
    "# Note: IN SAS dates are a special case of numeric values. Each day is assigned a specific numeric value starting from 1st January 1960.\n",
    "# Source: https://www.tutorialspoint.com/sas/sas_dates_times.htm\n",
    "\n",
    "# Convert to Integer\n",
    "for col in to_int_cols:\n",
    "    pass\n",
    "\n",
    "# Transform date time columns:\n",
    "for col in sas_df.select_dtypes(include=['float64']):\n",
    "    # From SAS date to Python datetime64\n",
    "    if col in ['arrdate', 'depdate']:\n",
    "        new_df[[key]] = pd.to_datetime(new_df[key], infer_datetime_format=True, errors='coerce', origin=pd.Timestamp('1960-01-01'))\n",
    "        #sas_df[[key]] = sas_df[key].values.strftime('%Y-%m-%d')\n",
    "        #new_df[[key]] = pd.to_datetime(sas_df[key], unit='D', origin=pd.Timestamp('1960-01-01'), errors='coerce')\n",
    "    # From Float64 to Integer\n",
    "    if col in ['i94yr', 'i94mon']:\n",
    "        new_df[[key]] = new_df[[key]].astype(int)\n",
    "    # From String \"YYYYMMDD\" to datetime\n",
    "    if col in ['dtadfile']:\n",
    "        new_df[[key]] = pd.to_numeric(new_df[key], errors='coerce')\n",
    "        new_df[[key]] = new_df[[key]].fillna(0)\n",
    "    if col in ['dtaddto']:\n",
    "        new_df[[key]] = pd.to_numeric(new_df[key], errors='coerce')\n",
    "        new_df[[key]] = new_df[[key]].fillna(0)\n",
    "\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.000000e+03\n",
       "mean     5.909467e+10\n",
       "std      1.507924e+10\n",
       "min      6.644910e+08\n",
       "25%      5.542381e+10\n",
       "50%      5.543718e+10\n",
       "75%      5.545536e+10\n",
       "max      9.251651e+10\n",
       "Name: admnum, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_df['admnum'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
